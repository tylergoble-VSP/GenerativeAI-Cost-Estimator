{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 00: Test Ollama Connection\n",
        "\n",
        "This notebook helps diagnose and fix Ollama connection issues.\n",
        "\n",
        "It will:\n",
        "1. Check if Ollama is running\n",
        "2. Test connectivity to the Ollama endpoint\n",
        "3. Verify the embedding model is available\n",
        "4. Provide troubleshooting steps if there are issues\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modules imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary modules\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import socket\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# Add the src directory to Python path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(\"Modules imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Check if Ollama Process is Running\n",
        "\n",
        "First, let's check if Ollama is actually running on the system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Ollama is running!\n",
            "   Found 1 Ollama process(es):\n",
            "   - Process ID: 18427\n",
            "   - Command: /usr/local/bin/ollama serve\n"
          ]
        }
      ],
      "source": [
        "# Check if Ollama process is running\n",
        "# We use 'pgrep' command to search for processes with 'ollama' in their name\n",
        "# pgrep returns process IDs (PIDs) if found, or nothing if not found\n",
        "try:\n",
        "    # Run the pgrep command to find ollama processes\n",
        "    # subprocess.run() executes a shell command and returns the result\n",
        "    result = subprocess.run(\n",
        "        ['pgrep', '-f', 'ollama'],  # -f means search in full command line, not just process name\n",
        "        capture_output=True,        # Capture both stdout and stderr\n",
        "        text=True                   # Return output as text string, not bytes\n",
        "    )\n",
        "    \n",
        "    # Check if any processes were found (result.returncode == 0 means success)\n",
        "    if result.returncode == 0 and result.stdout.strip():\n",
        "        # Split the output by newlines to get individual process IDs\n",
        "        pids = result.stdout.strip().split('\\n')\n",
        "        print(f\"âœ… Ollama is running!\")\n",
        "        print(f\"   Found {len(pids)} Ollama process(es):\")\n",
        "        for pid in pids:\n",
        "            print(f\"   - Process ID: {pid}\")\n",
        "            \n",
        "        # Try to get more info about the process using 'ps' command\n",
        "        # This shows the full command that started the process\n",
        "        for pid in pids:\n",
        "            ps_result = subprocess.run(\n",
        "                ['ps', '-p', pid, '-o', 'cmd='],  # Get command line for this PID\n",
        "                capture_output=True,\n",
        "                text=True\n",
        "            )\n",
        "            if ps_result.returncode == 0:\n",
        "                print(f\"   - Command: {ps_result.stdout.strip()}\")\n",
        "    else:\n",
        "        print(\"âŒ Ollama is NOT running\")\n",
        "        print(\"   No Ollama processes found on the system\")\n",
        "        \n",
        "except FileNotFoundError:\n",
        "    # pgrep might not be available on all systems\n",
        "    print(\"âš ï¸  Could not check for Ollama process (pgrep not found)\")\n",
        "    print(\"   Trying alternative method...\")\n",
        "    \n",
        "    # Alternative: try using 'ps' command directly\n",
        "    try:\n",
        "        ps_result = subprocess.run(\n",
        "            ['ps', 'aux'],  # List all processes\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "        if 'ollama' in ps_result.stdout.lower():\n",
        "            print(\"âœ… Found 'ollama' in process list\")\n",
        "            # Extract lines containing ollama\n",
        "            for line in ps_result.stdout.split('\\n'):\n",
        "                if 'ollama' in line.lower():\n",
        "                    print(f\"   {line.strip()}\")\n",
        "        else:\n",
        "            print(\"âŒ Ollama is NOT running\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Could not check processes: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Test Network Connectivity to Ollama Port\n",
        "\n",
        "Check if we can connect to port 11434 (Ollama's default port) on localhost.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing connection to localhost:11434...\n",
            "âœ… Port 11434 is open and accepting connections on localhost\n",
            "\n",
            "Testing connection to 127.0.0.1:11434...\n",
            "âœ… Port 11434 is open on 127.0.0.1\n"
          ]
        }
      ],
      "source": [
        "# Test if we can connect to port 11434 on localhost\n",
        "# This checks if something is listening on that port\n",
        "def test_port_connection(host, port, timeout=2):\n",
        "    \"\"\"\n",
        "    Test if we can connect to a specific host and port.\n",
        "    \n",
        "    Args:\n",
        "        host: The hostname or IP address (e.g., 'localhost' or '127.0.0.1')\n",
        "        port: The port number to test (e.g., 11434)\n",
        "        timeout: How long to wait before giving up (in seconds)\n",
        "    \n",
        "    Returns:\n",
        "        True if connection successful, False otherwise\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a socket object\n",
        "        # socket.AF_INET means IPv4, socket.SOCK_STREAM means TCP\n",
        "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "        \n",
        "        # Set a timeout so we don't wait forever\n",
        "        sock.settimeout(timeout)\n",
        "        \n",
        "        # Try to connect to the host and port\n",
        "        # This will raise an exception if connection fails\n",
        "        result = sock.connect_ex((host, port))\n",
        "        \n",
        "        # Close the socket\n",
        "        sock.close()\n",
        "        \n",
        "        # connect_ex returns 0 if connection successful, non-zero if failed\n",
        "        return result == 0\n",
        "        \n",
        "    except Exception as e:\n",
        "        # If anything goes wrong, return False\n",
        "        print(f\"   Error testing connection: {e}\")\n",
        "        return False\n",
        "\n",
        "# Test localhost:11434\n",
        "print(\"Testing connection to localhost:11434...\")\n",
        "host = \"localhost\"\n",
        "port = 11434\n",
        "\n",
        "if test_port_connection(host, port):\n",
        "    print(f\"âœ… Port {port} is open and accepting connections on {host}\")\n",
        "else:\n",
        "    print(f\"âŒ Cannot connect to {host}:{port}\")\n",
        "    print(f\"   This means either:\")\n",
        "    print(f\"   1. Ollama is not running\")\n",
        "    print(f\"   2. Ollama is running on a different port\")\n",
        "    print(f\"   3. Ollama is running on a different host\")\n",
        "    \n",
        "# Also test 127.0.0.1 (sometimes different from localhost)\n",
        "print(f\"\\nTesting connection to 127.0.0.1:{port}...\")\n",
        "if test_port_connection(\"127.0.0.1\", port):\n",
        "    print(f\"âœ… Port {port} is open on 127.0.0.1\")\n",
        "else:\n",
        "    print(f\"âŒ Cannot connect to 127.0.0.1:{port}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Test HTTP Connection to Ollama API\n",
        "\n",
        "Try to actually make an HTTP request to Ollama's API to see if it responds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Ollama API endpoints...\n",
            "\n",
            "Testing: http://localhost:11434\n",
            "  âœ… Connection successful!\n",
            "\n",
            "âœ… Found working Ollama endpoint: http://localhost:11434\n",
            "\n",
            "ðŸ’¡ Update your config to use: ollama_endpoint=\"http://localhost:11434\"\n"
          ]
        }
      ],
      "source": [
        "# Test different possible Ollama endpoints\n",
        "# Since you're on SSH, Ollama might be on a different address\n",
        "possible_endpoints = [\n",
        "    \"http://localhost:11434\",\n",
        "    \"http://127.0.0.1:11434\",\n",
        "    # If you're on a cluster, might need the actual hostname\n",
        "    # Add more endpoints here if needed\n",
        "]\n",
        "\n",
        "def test_ollama_endpoint(base_url):\n",
        "    \"\"\"\n",
        "    Test if we can reach Ollama's API at a given endpoint.\n",
        "    \n",
        "    Args:\n",
        "        base_url: The base URL to test (e.g., \"http://localhost:11434\")\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (success: bool, message: str, response_data: dict or None)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try to access Ollama's /api/tags endpoint\n",
        "        # This endpoint lists available models and doesn't require any input\n",
        "        url = f\"{base_url}/api/tags\"\n",
        "        \n",
        "        # Make a GET request with a short timeout\n",
        "        # GET is used because we're just asking for information, not sending data\n",
        "        response = requests.get(url, timeout=5)\n",
        "        \n",
        "        # Check if the request was successful (HTTP 200)\n",
        "        if response.status_code == 200:\n",
        "            # Parse the JSON response\n",
        "            data = response.json()\n",
        "            return True, \"âœ… Connection successful!\", data\n",
        "        else:\n",
        "            # Got a response but it's an error\n",
        "            return False, f\"âŒ Got HTTP {response.status_code}: {response.text}\", None\n",
        "            \n",
        "    except requests.exceptions.ConnectionError as e:\n",
        "        # Connection refused or couldn't reach the server\n",
        "        return False, f\"âŒ Connection refused - Ollama not running at {base_url}\", None\n",
        "    except requests.exceptions.Timeout:\n",
        "        # Request took too long\n",
        "        return False, f\"âŒ Connection timeout - server not responding\", None\n",
        "    except Exception as e:\n",
        "        # Some other error\n",
        "        return False, f\"âŒ Error: {str(e)}\", None\n",
        "\n",
        "# Test each possible endpoint\n",
        "print(\"Testing Ollama API endpoints...\\n\")\n",
        "working_endpoint = None\n",
        "\n",
        "for endpoint in possible_endpoints:\n",
        "    print(f\"Testing: {endpoint}\")\n",
        "    success, message, data = test_ollama_endpoint(endpoint)\n",
        "    print(f\"  {message}\")\n",
        "    \n",
        "    if success and data:\n",
        "        working_endpoint = endpoint\n",
        "        # Show available models\n",
        "        models = data.get('models', [])\n",
        "        if models:\n",
        "            print(f\"  Available models ({len(models)}):\")\n",
        "            for model in models[:5]:  # Show first 5 models\n",
        "                model_name = model.get('name', 'unknown')\n",
        "                print(f\"    - {model_name}\")\n",
        "            if len(models) > 5:\n",
        "                print(f\"    ... and {len(models) - 5} more\")\n",
        "        print()\n",
        "        break\n",
        "    print()\n",
        "\n",
        "if working_endpoint:\n",
        "    print(f\"âœ… Found working Ollama endpoint: {working_endpoint}\")\n",
        "    print(f\"\\nðŸ’¡ Update your config to use: ollama_endpoint=\\\"{working_endpoint}\\\"\")\n",
        "else:\n",
        "    print(\"âŒ No working Ollama endpoint found\")\n",
        "    print(\"\\nðŸ’¡ Next steps:\")\n",
        "    print(\"   1. Make sure Ollama is installed\")\n",
        "    print(\"   2. Start Ollama with: ollama serve\")\n",
        "    print(\"   3. If running on a different host/port, update the endpoint in your config\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Test Embedding Model Availability\n",
        "\n",
        "Check if the embedding model we need is available and can be used.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing embedding model: embeddinggemma\n",
            "Using endpoint: http://localhost:11434\n",
            "\n",
            "Test text: 'This is a test sentence for embedding.'\n",
            "Calling Ollama embedding API...\n",
            "âœ… Embedding successful!\n",
            "   Embedding dimension: 768\n",
            "   First 5 values: [-0.1562882512807846, -0.0013628721935674548, 0.04643210023641586, 0.005612329114228487, -0.018335139378905296]\n",
            "   Model used: None\n",
            "   Token usage: {}\n",
            "\n",
            "âœ… Everything is working! You can proceed with your notebook.\n"
          ]
        }
      ],
      "source": [
        "# Test if we can actually get an embedding from Ollama\n",
        "# This is the real test - can we use the embedding model?\n",
        "\n",
        "if not working_endpoint:\n",
        "    print(\"âš ï¸  Skipping embedding test - no working endpoint found\")\n",
        "    print(\"   Please fix the connection issue first\")\n",
        "else:\n",
        "    # Import the embedding function from our code\n",
        "    from src.ollama_client import get_embedding\n",
        "    \n",
        "    # The model name we want to use\n",
        "    model_name = \"embeddinggemma\"\n",
        "    \n",
        "    print(f\"Testing embedding model: {model_name}\")\n",
        "    print(f\"Using endpoint: {working_endpoint}\\n\")\n",
        "    \n",
        "    try:\n",
        "        # Try to get an embedding for a simple test text\n",
        "        # This is a small text to test if the model works\n",
        "        test_text = \"This is a test sentence for embedding.\"\n",
        "        \n",
        "        print(f\"Test text: '{test_text}'\")\n",
        "        print(\"Calling Ollama embedding API...\")\n",
        "        \n",
        "        # Call the embedding function\n",
        "        # This will make a POST request to Ollama's /api/embeddings endpoint\n",
        "        embedding, metadata = get_embedding(test_text, model_name, working_endpoint)\n",
        "        \n",
        "        # If we get here, it worked!\n",
        "        print(f\"âœ… Embedding successful!\")\n",
        "        print(f\"   Embedding dimension: {len(embedding)}\")\n",
        "        print(f\"   First 5 values: {embedding[:5]}\")\n",
        "        print(f\"   Model used: {metadata.get('model', 'unknown')}\")\n",
        "        \n",
        "        if 'usage' in metadata:\n",
        "            usage = metadata['usage']\n",
        "            print(f\"   Token usage: {usage}\")\n",
        "        \n",
        "        print(f\"\\nâœ… Everything is working! You can proceed with your notebook.\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Embedding test failed: {e}\")\n",
        "        print(f\"\\nPossible issues:\")\n",
        "        print(f\"   1. Model '{model_name}' is not installed\")\n",
        "        print(f\"      Try: ollama pull {model_name}\")\n",
        "        print(f\"   2. Model name is incorrect\")\n",
        "        print(f\"      Check available models in Step 3 above\")\n",
        "        print(f\"   3. Network or permission issues\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting Guide\n",
        "\n",
        "If Ollama is not working, here are common solutions:\n",
        "\n",
        "### 1. Start Ollama Service\n",
        "If Ollama is not running, start it with:\n",
        "```bash\n",
        "ollama serve\n",
        "```\n",
        "\n",
        "### 2. Check if Ollama is Installed\n",
        "```bash\n",
        "which ollama\n",
        "ollama --version\n",
        "```\n",
        "\n",
        "### 3. Pull the Required Model\n",
        "If the model is not available:\n",
        "```bash\n",
        "ollama pull embeddinggemma\n",
        "```\n",
        "\n",
        "### 4. Check for Different Host/Port\n",
        "If you're on a cluster or using SSH with port forwarding:\n",
        "- Check what hostname/IP Ollama is actually running on\n",
        "- Check if port forwarding is needed\n",
        "- Update the `ollama_endpoint` in your config accordingly\n",
        "\n",
        "### 5. Check Firewall/Security Groups\n",
        "Make sure port 11434 is not blocked by firewall rules.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Fix: Start Ollama\n",
        "\n",
        "If Ollama is not running, you can start it from this notebook. **Note:** You'll need to run this in a terminal separately since Ollama runs as a long-running service.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell shows you how to start Ollama\n",
        "# Since Ollama needs to run continuously, you should start it in a separate terminal\n",
        "\n",
        "print(\"To start Ollama, open a NEW terminal window and run:\")\n",
        "print(\"  ollama serve\")\n",
        "print()\n",
        "print(\"Or if Ollama is not in your PATH, try:\")\n",
        "print(\"  ~/.local/bin/ollama serve\")\n",
        "print(\"  /usr/local/bin/ollama serve\")\n",
        "print()\n",
        "print(\"After starting Ollama, come back to this notebook and re-run the cells above\")\n",
        "print(\"to verify the connection is working.\")\n",
        "print()\n",
        "print(\"ðŸ’¡ TIP: You can run Ollama in the background with:\")\n",
        "print(\"  nohup ollama serve > ollama.log 2>&1 &\")\n",
        "print()\n",
        "print(\"This will:\")\n",
        "print(\"  - Run Ollama in the background (nohup)\")\n",
        "print(\"  - Save output to ollama.log\")\n",
        "print(\"  - Continue running even if you close the terminal\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
