{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diagnose Ollama 404 Error\n",
        "\n",
        "This notebook helps diagnose why you're getting a 404 error when calling the Ollama API.\n",
        "\n",
        "## What the Error Means\n",
        "\n",
        "A 404 error means \"Not Found\" - the server couldn't find what you asked for. This could mean:\n",
        "1. The model doesn't exist in your Ollama installation\n",
        "2. Ollama server is not running or not accessible\n",
        "3. The API endpoint path is wrong (unlikely if you're using the latest Ollama)\n",
        "4. The model name format is incorrect\n",
        "\n",
        "Let's run some diagnostic tests to figure out what's wrong.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Add the src directory to Python path so we can import our modules\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "from src.config import Config\n",
        "\n",
        "print(\"‚úÖ Imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 1: Check if Ollama Server is Running\n",
        "\n",
        "First, let's verify that Ollama is running and accessible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test if Ollama server is running\n",
        "# We'll try to access the /api/tags endpoint which lists available models\n",
        "base_url = \"http://localhost:11434\"\n",
        "test_url = f\"{base_url}/api/tags\"\n",
        "\n",
        "print(f\"Testing Ollama connection at: {base_url}\")\n",
        "print(f\"Endpoint: {test_url}\")\n",
        "print()\n",
        "\n",
        "try:\n",
        "    # Make a GET request to the tags endpoint\n",
        "    # GET is used because we're just asking for information, not sending data\n",
        "    response = requests.get(test_url, timeout=5)\n",
        "    \n",
        "    # Check the HTTP status code\n",
        "    # 200 means success, anything else is an error\n",
        "    if response.status_code == 200:\n",
        "        print(\"‚úÖ Ollama server is running and accessible!\")\n",
        "        print()\n",
        "        \n",
        "        # Parse the JSON response to see what models are available\n",
        "        data = response.json()\n",
        "        models = data.get('models', [])\n",
        "        \n",
        "        print(f\"Found {len(models)} available models:\")\n",
        "        for i, model in enumerate(models, 1):\n",
        "            model_name = model.get('name', 'Unknown')\n",
        "            print(f\"  {i}. {model_name}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Ollama returned status code: {response.status_code}\")\n",
        "        print(f\"Response: {response.text}\")\n",
        "        \n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(\"‚ùå Could not connect to Ollama server!\")\n",
        "    print(f\"   Make sure Ollama is running at {base_url}\")\n",
        "    print(\"   Start it with: ollama serve\")\n",
        "except requests.exceptions.Timeout:\n",
        "    print(\"‚ùå Connection to Ollama timed out!\")\n",
        "    print(\"   The server might be overloaded or not responding\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {type(e).__name__}: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 2: Check if the Chat API Endpoint Exists\n",
        "\n",
        "Now let's test if the `/api/chat` endpoint is available. This is the endpoint your code uses for text generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the /api/chat endpoint\n",
        "# This is the endpoint used by the generate() function in ollama_client.py\n",
        "chat_url = f\"{base_url}/api/chat\"\n",
        "\n",
        "print(f\"Testing chat endpoint: {chat_url}\")\n",
        "print()\n",
        "\n",
        "# First, let's get a list of available models to test with\n",
        "try:\n",
        "    tags_response = requests.get(f\"{base_url}/api/tags\", timeout=5)\n",
        "    if tags_response.status_code == 200:\n",
        "        models_data = tags_response.json()\n",
        "        available_models = [m.get('name') for m in models_data.get('models', [])]\n",
        "        \n",
        "        if available_models:\n",
        "            # Use the first available model for testing\n",
        "            test_model = available_models[0]\n",
        "            print(f\"Using model '{test_model}' for testing\")\n",
        "            print()\n",
        "            \n",
        "            # Prepare a test request\n",
        "            # This matches the format used in ollama_client.py\n",
        "            test_payload = {\n",
        "                \"model\": test_model,\n",
        "                \"messages\": [\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": \"Say hello\"\n",
        "                    }\n",
        "                ],\n",
        "                \"stream\": False  # Don't stream, get full response at once\n",
        "            }\n",
        "            \n",
        "            print(f\"Sending test request to {chat_url}\")\n",
        "            print(f\"Payload: {json.dumps(test_payload, indent=2)}\")\n",
        "            print()\n",
        "            \n",
        "            # Send the request\n",
        "            chat_response = requests.post(chat_url, json=test_payload, timeout=30)\n",
        "            \n",
        "            # Check the response\n",
        "            if chat_response.status_code == 200:\n",
        "                print(\"‚úÖ Chat endpoint is working!\")\n",
        "                result = chat_response.json()\n",
        "                print(f\"Response: {result.get('message', {}).get('content', 'No content')[:100]}...\")\n",
        "            elif chat_response.status_code == 404:\n",
        "                print(\"‚ùå 404 Not Found - The chat endpoint doesn't exist!\")\n",
        "                print(\"   This might mean:\")\n",
        "                print(\"   1. Your Ollama version is too old (upgrade with: ollama update)\")\n",
        "                print(\"   2. The endpoint path is wrong\")\n",
        "                print(f\"   Response: {chat_response.text[:200]}\")\n",
        "            else:\n",
        "                print(f\"‚ùå Got status code: {chat_response.status_code}\")\n",
        "                print(f\"Response: {chat_response.text[:200]}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  No models found in Ollama. You need to pull a model first.\")\n",
        "            print(\"   Example: ollama pull gemma3:1b\")\n",
        "    else:\n",
        "        print(f\"‚ùå Could not get model list (status {tags_response.status_code})\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error testing chat endpoint: {type(e).__name__}: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if the specific model from your experiment exists\n",
        "# The experiment name shows \"gengemma3_1b\" but the actual model name should be \"gemma3:1b\"\n",
        "experiment_model_name = \"gemma3:1b\"  # This is what should be in the config\n",
        "\n",
        "print(f\"Checking if model '{experiment_model_name}' exists...\")\n",
        "print()\n",
        "\n",
        "try:\n",
        "    # Get list of available models\n",
        "    tags_response = requests.get(f\"{base_url}/api/tags\", timeout=5)\n",
        "    if tags_response.status_code == 200:\n",
        "        models_data = tags_response.json()\n",
        "        available_models = [m.get('name') for m in models_data.get('models', [])]\n",
        "        \n",
        "        # Check if our model is in the list\n",
        "        if experiment_model_name in available_models:\n",
        "            print(f\"‚úÖ Model '{experiment_model_name}' is available!\")\n",
        "            \n",
        "            # Try to use it\n",
        "            print(f\"\\nTesting the model with a simple request...\")\n",
        "            test_payload = {\n",
        "                \"model\": experiment_model_name,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": \"test\"}],\n",
        "                \"stream\": False\n",
        "            }\n",
        "            \n",
        "            test_response = requests.post(chat_url, json=test_payload, timeout=30)\n",
        "            \n",
        "            if test_response.status_code == 200:\n",
        "                print(\"‚úÖ Model works correctly!\")\n",
        "            else:\n",
        "                print(f\"‚ùå Model exists but request failed with status {test_response.status_code}\")\n",
        "                print(f\"Response: {test_response.text[:200]}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Model '{experiment_model_name}' is NOT available!\")\n",
        "            print(f\"\\nAvailable models are:\")\n",
        "            for model in available_models:\n",
        "                print(f\"  - {model}\")\n",
        "            print(f\"\\nüí° To install the model, run: ollama pull {experiment_model_name}\")\n",
        "            \n",
        "            # Check for similar model names\n",
        "            similar = [m for m in available_models if 'gemma' in m.lower() or '1b' in m.lower()]\n",
        "            if similar:\n",
        "                print(f\"\\nSimilar models found:\")\n",
        "                for model in similar:\n",
        "                    print(f\"  - {model}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Could not get model list\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {type(e).__name__}: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 4: Test with the Actual Code\n",
        "\n",
        "Let's test using the actual `generate()` function from your codebase to see what error message we get now (with improved error handling).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the generate function from ollama_client\n",
        "from src.ollama_client import generate\n",
        "\n",
        "# Test with the actual function\n",
        "test_model = \"gemma3:1b\"  # The model from your experiment\n",
        "test_prompt = \"Say hello in one word\"\n",
        "\n",
        "print(f\"Testing generate() function with:\")\n",
        "print(f\"  Model: {test_model}\")\n",
        "print(f\"  Prompt: {test_prompt}\")\n",
        "print(f\"  Base URL: {base_url}\")\n",
        "print()\n",
        "\n",
        "try:\n",
        "    # Call the generate function\n",
        "    # This is the same function that failed in your experiment\n",
        "    response_text, metadata = generate(\n",
        "        prompt=test_prompt,\n",
        "        model=test_model,\n",
        "        base_url=base_url,\n",
        "        stream=False\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ generate() function worked!\")\n",
        "    print(f\"Response: {response_text}\")\n",
        "    print(f\"Metadata: {json.dumps(metadata, indent=2)}\")\n",
        "    \n",
        "except requests.exceptions.HTTPError as e:\n",
        "    print(f\"‚ùå HTTP Error occurred:\")\n",
        "    print(f\"   {str(e)}\")\n",
        "    print()\n",
        "    print(\"This is the same type of error you saw in your experiment.\")\n",
        "    print(\"The improved error handling should now show more details about what went wrong.\")\n",
        "    \n",
        "except ConnectionError as e:\n",
        "    print(f\"‚ùå Connection Error:\")\n",
        "    print(f\"   {str(e)}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Unexpected error: {type(e).__name__}: {str(e)}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
