{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 08: Pricing Break-Even Analysis\n",
    "\n",
    "This notebook performs a comprehensive pricing sensitivity analysis to find break-even points between API and local pricing for each model combination.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Full Grid Search**: Complete coverage of pricing space (49,100 combinations)\n",
    "- **Individual Data Points**: Uses actual experiment results, not aggregated means\n",
    "- **Break-Even Detection**: Accurate identification of break-even points\n",
    "- **Comprehensive Visualization**: Multiple plot types for different insights\n",
    "- **Model-Specific Analysis**: Separate break-even analysis for each model combination\n",
    "\n",
    "## Pricing Ranges\n",
    "\n",
    "- **API Pricing**: $0.0001 to $0.01 per 1k tokens (increments of $0.0001) = 100 values\n",
    "- **Local Hourly Pricing**: $0.1 to $5.0 per hour (increments of $0.01) = 491 values\n",
    "- **Total Combinations**: 100 \u00d7 491 = 49,100 pricing combinations per model\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Load experiment data in Section 1\n",
    "2. Define pricing grid in Section 2\n",
    "3. Run break-even analysis in Section 4\n",
    "4. View visualizations in Section 6\n",
    "5. Export results in Section 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Data Loading\n",
    "\n",
    "Import libraries and load experiment data with individual data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 All modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Interpolation for smooth curves\n",
    "from scipy import interpolate\n",
    "\n",
    "# Add the src directory to Python path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(\"\u2705 All modules imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Configuration loaded\n",
      "   Analysis directory: /home/goble54/spark-dev-workspace/GenerativeAI-Cost-Estimator/notebooks/results/pricing_break_even/analysis_20251123_235446\n",
      "   Experiment timestamp: latest\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: Data Source Selection\n",
    "# ============================================================================\n",
    "# Specify which experiment run to analyze\n",
    "# Options:\n",
    "#   1. Specify exact timestamp (e.g., \"20251123_164759\")\n",
    "#   2. Use \"latest\" to automatically find most recent\n",
    "#   3. Specify full path to CSV file\n",
    "\n",
    "EXPERIMENT_TIMESTAMP = \"latest\"  # Change to specific timestamp or path if needed\n",
    "EXPERIMENT_CSV_PATH = None  # Set to None to use automatic \"latest\" detection, or specify full path to CSV file\n",
    "\n",
    "# Results directory\n",
    "RESULTS_DIR = project_root / \"notebooks\" / \"results\" / \"pricing_break_even\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create timestamped subdirectory for this analysis\n",
    "analysis_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "ANALYSIS_DIR = RESULTS_DIR / f\"analysis_{analysis_timestamp}\"\n",
    "ANALYSIS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\u2705 Configuration loaded\")\n",
    "print(f\"   Analysis directory: {ANALYSIS_DIR}\")\n",
    "print(f\"   Experiment timestamp: {EXPERIMENT_TIMESTAMP}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcc1 Using latest experiment data:\n",
      "   Experiment directory: nolh\n",
      "   CSV exports directory: csv_exports_20251123_164759\n",
      "   CSV file: 01_full_experiment_summary_20251123_164759.csv\n",
      "   Modified: 2025-11-23 16:48:14\n",
      "\n",
      "\u2705 Loaded experiment data:\n",
      "   Total experiments: 30\n",
      "   Columns: 40\n",
      "\n",
      "\ud83d\udcca Data preview:\n",
      "             export_timestamp export_timestamp_readable  \\\n",
      "0  2025-11-23T16:47:59.732137       2025-11-23 16:47:59   \n",
      "1  2025-11-23T16:47:59.732137       2025-11-23 16:47:59   \n",
      "2  2025-11-23T16:47:59.732137       2025-11-23 16:47:59   \n",
      "3  2025-11-23T16:47:59.732137       2025-11-23 16:47:59   \n",
      "4  2025-11-23T16:47:59.732137       2025-11-23 16:47:59   \n",
      "\n",
      "                                     experiment_name     gen_model  \\\n",
      "0  nolh_docs43_chunk326_q4_gengpt_oss_120b_embsno...  gpt-oss:120b   \n",
      "1  nolh_docs17_chunk1783_q7_gengemma3_4b_embnomic...     gemma3:4b   \n",
      "2  nolh_docs16_chunk2137_q4_genqwen3_latest_embem...  qwen3:latest   \n",
      "3  nolh_docs45_chunk1560_q9_gengemma3_4b_embnomic...     gemma3:4b   \n",
      "4  nolh_docs11_chunk2708_q5_gengemma3_12b_embqwen...    gemma3:12b   \n",
      "\n",
      "                       embed_model  num_docs  chunk_size  chunk_overlap  \\\n",
      "0     snowflake-arctic-embed2:568m        43         326             32   \n",
      "1  nomic-embed-text:137m-v1.5-fp16        17        1783             50   \n",
      "2                   embeddinggemma        16        2137             50   \n",
      "3  nomic-embed-text:137m-v1.5-fp16        45        1560             50   \n",
      "4          qwen3-embedding:8b-fp16        11        2708             50   \n",
      "\n",
      "   num_questions_per_doc  total_questions  ...  cost_per_1k_tokens  \\\n",
      "0                      4              172  ...              0.0025   \n",
      "1                      7              119  ...              0.0025   \n",
      "2                      4               64  ...              0.0025   \n",
      "3                      9              405  ...              0.0025   \n",
      "4                      5               55  ...              0.0025   \n",
      "\n",
      "   cost_per_document  cost_per_question  pricing_mode  nolh_sample_idx  \\\n",
      "0           0.001211           0.000303          both                0   \n",
      "1           0.001226           0.000175          both                1   \n",
      "2           0.001223           0.000306          both                2   \n",
      "3           0.001213           0.000135          both                3   \n",
      "4           0.001218           0.000244          both                4   \n",
      "\n",
      "   nolh_num_docs  nolh_chunk_size  nolh_num_questions  num_docs_range  \\\n",
      "0             43              326                   4           41-50   \n",
      "1             17             1783                   7           11-20   \n",
      "2             16             2137                   4           11-20   \n",
      "3             45             1560                   9           41-50   \n",
      "4             11             2708                   5           11-20   \n",
      "\n",
      "   chunk_size_range  \n",
      "0             0-500  \n",
      "1         1001-2000  \n",
      "2         2001-3000  \n",
      "3         1001-2000  \n",
      "4         2001-3000  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load full experiment summary CSV\n",
    "# This contains individual experiment data points (not aggregated)\n",
    "\n",
    "if EXPERIMENT_CSV_PATH:\n",
    "    # Use specified path (resolve relative to project root)\n",
    "    summary_csv_path = (project_root / EXPERIMENT_CSV_PATH).resolve()\n",
    "    if not summary_csv_path.exists():\n",
    "        raise FileNotFoundError(f\"CSV file not found: {summary_csv_path}\")\n",
    "    print(f\"\ud83d\udcc4 Loading from specified path: {summary_csv_path}\")\n",
    "else:\n",
    "    # Find experiment directory\n",
    "    experiments_base = project_root / \"notebooks\" / \"results\" / \"experiments\"\n",
    "    \n",
    "    if EXPERIMENT_TIMESTAMP == \"latest\":\n",
    "        # Strategy: Find all CSV files first, then pick the most recent one\n",
    "        # This ensures we get a directory that actually has data\n",
    "        \n",
    "        # Search for all full experiment summary CSV files\n",
    "        all_csv_files = []\n",
    "        \n",
    "        # Search in new structure: nolh_*/csv_exports_*/\n",
    "        for nolh_dir in experiments_base.glob(\"nolh_*\"):\n",
    "            if nolh_dir.is_dir():\n",
    "                dir_timestamp = nolh_dir.name.split('_', 1)[1] if '_' in nolh_dir.name else None\n",
    "                csv_exports_dir = nolh_dir / f\"csv_exports_{dir_timestamp}\" if dir_timestamp else nolh_dir / \"csv_exports\"\n",
    "                csv_files = list(csv_exports_dir.glob(\"01_full_experiment_summary_*.csv\"))\n",
    "                if not csv_files:\n",
    "                    csv_files = list(csv_exports_dir.glob(\"01_full_experiment_summary.csv\"))\n",
    "                for csv_file in csv_files:\n",
    "                    all_csv_files.append((csv_file, nolh_dir, csv_exports_dir))\n",
    "        \n",
    "        # Search in old structure: nolh/csv_exports_*/\n",
    "        nolh_base_dir = experiments_base / \"nolh\"\n",
    "        if nolh_base_dir.exists():\n",
    "            for csv_exports_dir in nolh_base_dir.glob(\"csv_exports*\"):\n",
    "                if csv_exports_dir.is_dir():\n",
    "                    csv_files = list(csv_exports_dir.glob(\"01_full_experiment_summary_*.csv\"))\n",
    "                    if not csv_files:\n",
    "                        csv_files = list(csv_exports_dir.glob(\"01_full_experiment_summary.csv\"))\n",
    "                    for csv_file in csv_files:\n",
    "                        all_csv_files.append((csv_file, nolh_base_dir, csv_exports_dir))\n",
    "        \n",
    "        if not all_csv_files:\n",
    "            raise FileNotFoundError(\"No full experiment summary CSV files found in any experiment directory!\")\n",
    "        \n",
    "        # Sort by file modification time (most recent first)\n",
    "        all_csv_files.sort(key=lambda x: x[0].stat().st_mtime, reverse=True)\n",
    "        \n",
    "        # Use the most recent CSV file\n",
    "        summary_csv_path, experiment_dir, csv_exports_dir = all_csv_files[0]\n",
    "        print(f\"\ud83d\udcc1 Using latest experiment data:\")\n",
    "        print(f\"   Experiment directory: {experiment_dir.name}\")\n",
    "        print(f\"   CSV exports directory: {csv_exports_dir.name}\")\n",
    "        print(f\"   CSV file: {summary_csv_path.name}\")\n",
    "        print(f\"   Modified: {datetime.fromtimestamp(summary_csv_path.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    else:\n",
    "        # Use specified timestamp\n",
    "        # Try new structure first\n",
    "        experiment_dir = experiments_base / f\"nolh_{EXPERIMENT_TIMESTAMP}\"\n",
    "        if experiment_dir.exists():\n",
    "            print(f\"\ud83d\udcc1 Using experiment (new structure): {experiment_dir.name}\")\n",
    "            csv_exports_dir = experiment_dir / f\"csv_exports_{EXPERIMENT_TIMESTAMP}\"\n",
    "        else:\n",
    "            # Try old structure\n",
    "            nolh_base_dir = experiments_base / \"nolh\"\n",
    "            csv_exports_dir = nolh_base_dir / f\"csv_exports_{EXPERIMENT_TIMESTAMP}\"\n",
    "            if not csv_exports_dir.exists():\n",
    "                raise FileNotFoundError(f\"Experiment directory not found: {experiment_dir} or {csv_exports_dir}\")\n",
    "            experiment_dir = nolh_base_dir\n",
    "            print(f\"\ud83d\udcc1 Using experiment (old structure): {csv_exports_dir.name}\")\n",
    "        \n",
    "        # Find the full experiment summary CSV\n",
    "        summary_csv_files = list(csv_exports_dir.glob(\"01_full_experiment_summary_*.csv\"))\n",
    "        \n",
    "        if not summary_csv_files:\n",
    "            # Try without timestamp in filename (old format)\n",
    "            summary_csv_files = list(csv_exports_dir.glob(\"01_full_experiment_summary.csv\"))\n",
    "        \n",
    "        if not summary_csv_files:\n",
    "            raise FileNotFoundError(f\"No full experiment summary CSV found in {csv_exports_dir}\")\n",
    "        \n",
    "        # Use most recent if multiple\n",
    "        summary_csv_path = sorted(summary_csv_files, reverse=True)[0]\n",
    "        print(f\"\ud83d\udcc4 Loading: {summary_csv_path.name}\")\n",
    "\n",
    "# Load the CSV\n",
    "experiment_df = pd.read_csv(summary_csv_path)\n",
    "print(f\"\\n\u2705 Loaded experiment data:\")\n",
    "print(f\"   Total experiments: {len(experiment_df)}\")\n",
    "print(f\"   Columns: {len(experiment_df.columns)}\")\n",
    "print(f\"\\n\ud83d\udcca Data preview:\")\n",
    "print(experiment_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcca Experiment Data Statistics:\n",
      "================================================================================\n",
      "\n",
      "Model Combinations:\n",
      "gen_model     embed_model                    \n",
      "gemma3:12b    embeddinggemma                     2\n",
      "              qwen3-embedding:8b-fp16            2\n",
      "gemma3:1b     nomic-embed-text:137m-v1.5-fp16    1\n",
      "              snowflake-arctic-embed2:568m       1\n",
      "gemma3:27b    embeddinggemma                     1\n",
      "              nomic-embed-text:137m-v1.5-fp16    1\n",
      "              qwen3-embedding:8b-fp16            1\n",
      "              snowflake-arctic-embed2:568m       3\n",
      "gemma3:4b     nomic-embed-text:137m-v1.5-fp16    3\n",
      "              qwen3-embedding:8b-fp16            1\n",
      "              snowflake-arctic-embed2:568m       2\n",
      "gpt-oss:120b  qwen3-embedding:8b-fp16            1\n",
      "              snowflake-arctic-embed2:568m       1\n",
      "gpt-oss:20b   qwen3-embedding:8b-fp16            3\n",
      "              snowflake-arctic-embed2:568m       1\n",
      "qwen3:latest  embeddinggemma                     2\n",
      "              qwen3-embedding:8b-fp16            2\n",
      "              snowflake-arctic-embed2:568m       2\n",
      "dtype: int64\n",
      "\n",
      "Total unique model combinations: 18\n",
      "\n",
      "Token Statistics:\n",
      "   Min: 2,969\n",
      "   Max: 24,313\n",
      "   Mean: 13,393\n",
      "   Median: 13,630\n",
      "\n",
      "Runtime Statistics (seconds):\n",
      "   Min: 14.06\n",
      "   Max: 576.73\n",
      "   Mean: 197.60\n",
      "   Median: 126.67\n",
      "\n",
      "\u2705 Data ready for analysis!\n",
      "   18 model combinations to analyze\n"
     ]
    }
   ],
   "source": [
    "# Verify required columns exist\n",
    "required_columns = ['gen_model', 'embed_model', 'total_tokens', 'total_runtime_seconds']\n",
    "missing_columns = [col for col in required_columns if col not in experiment_df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "# Display data statistics\n",
    "print(\"\ud83d\udcca Experiment Data Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nModel Combinations:\")\n",
    "model_combos = experiment_df.groupby(['gen_model', 'embed_model']).size()\n",
    "print(model_combos)\n",
    "print(f\"\\nTotal unique model combinations: {len(model_combos)}\")\n",
    "\n",
    "print(f\"\\nToken Statistics:\")\n",
    "print(f\"   Min: {experiment_df['total_tokens'].min():,.0f}\")\n",
    "print(f\"   Max: {experiment_df['total_tokens'].max():,.0f}\")\n",
    "print(f\"   Mean: {experiment_df['total_tokens'].mean():,.0f}\")\n",
    "print(f\"   Median: {experiment_df['total_tokens'].median():,.0f}\")\n",
    "\n",
    "print(f\"\\nRuntime Statistics (seconds):\")\n",
    "print(f\"   Min: {experiment_df['total_runtime_seconds'].min():.2f}\")\n",
    "print(f\"   Max: {experiment_df['total_runtime_seconds'].max():.2f}\")\n",
    "print(f\"   Mean: {experiment_df['total_runtime_seconds'].mean():.2f}\")\n",
    "print(f\"   Median: {experiment_df['total_runtime_seconds'].median():.2f}\")\n",
    "\n",
    "# Group by model for analysis\n",
    "model_groups = experiment_df.groupby(['gen_model', 'embed_model'])\n",
    "print(f\"\\n\u2705 Data ready for analysis!\")\n",
    "print(f\"   {len(model_groups)} model combinations to analyze\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Pricing Grid Definition\n",
    "\n",
    "Define the pricing ranges for API and local costs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRICING GRID DEFINITION\n",
    "# ============================================================================\n",
    "\n",
    "# API Pricing: $0.0001 to $0.01 per 1k tokens\n",
    "# Increments of $0.0001 = 100 values\n",
    "api_price_min = 0.0001\n",
    "api_price_max = 0.01\n",
    "api_price_increment = 0.0001\n",
    "api_prices = np.arange(api_price_min, api_price_max + api_price_increment, api_price_increment)\n",
    "\n",
    "# Local Hourly Pricing: $0.1 to $5.0 per hour\n",
    "# Increments of $0.01 = 491 values\n",
    "hourly_price_min = 0.1\n",
    "hourly_price_max = 5.0\n",
    "hourly_price_increment = 0.01\n",
    "hourly_prices = np.arange(hourly_price_min, hourly_price_max + hourly_price_increment, hourly_price_increment)\n",
    "\n",
    "# Create full grid using meshgrid for efficient computation\n",
    "API_GRID, HOURLY_GRID = np.meshgrid(api_prices, hourly_prices)\n",
    "\n",
    "print(\"\ud83d\udcb0 Pricing Grid Definition:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAPI Pricing Range:\")\n",
    "print(f\"   Min: ${api_price_min:.4f} per 1k tokens\")\n",
    "print(f\"   Max: ${api_price_max:.4f} per 1k tokens\")\n",
    "print(f\"   Increment: ${api_price_increment:.4f}\")\n",
    "print(f\"   Number of values: {len(api_prices)}\")\n",
    "\n",
    "print(f\"\\nLocal Hourly Pricing Range:\")\n",
    "print(f\"   Min: ${hourly_price_min:.2f} per hour\")\n",
    "print(f\"   Max: ${hourly_price_max:.2f} per hour\")\n",
    "print(f\"   Increment: ${hourly_price_increment:.2f}\")\n",
    "print(f\"   Number of values: {len(hourly_prices)}\")\n",
    "\n",
    "print(f\"\\nTotal Pricing Combinations:\")\n",
    "total_combinations = len(api_prices) * len(hourly_prices)\n",
    "print(f\"   {len(api_prices)} \u00d7 {len(hourly_prices)} = {total_combinations:,} combinations\")\n",
    "\n",
    "print(f\"\\n\u2705 Pricing grid created and ready for analysis!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Cost Calculation Functions\n",
    "\n",
    "Vectorized functions for efficient cost calculations across the pricing grid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COST CALCULATION FUNCTIONS\n",
    "# ============================================================================\n",
    "# These functions are vectorized to efficiently compute costs across the entire pricing grid\n",
    "\n",
    "def calculate_api_cost(tokens: float, api_price_per_1k: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate API cost for given tokens and API price.\n",
    "    \n",
    "    Args:\n",
    "        tokens: Number of tokens (scalar)\n",
    "        api_price_per_1k: Array of API prices per 1k tokens\n",
    "    \n",
    "    Returns:\n",
    "        Array of API costs for each price\n",
    "    \"\"\"\n",
    "    # API cost = (tokens / 1000) * api_price_per_1k\n",
    "    return (tokens / 1000.0) * api_price_per_1k\n",
    "\n",
    "\n",
    "def calculate_local_cost(runtime_seconds: float, hourly_price: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate local cost for given runtime and hourly price.\n",
    "    \n",
    "    Args:\n",
    "        runtime_seconds: Runtime in seconds (scalar)\n",
    "        hourly_price: Array of hourly prices\n",
    "    \n",
    "    Returns:\n",
    "        Array of local costs for each hourly price\n",
    "    \"\"\"\n",
    "    # Local cost = (runtime_seconds / 3600) * hourly_price\n",
    "    runtime_hours = runtime_seconds / 3600.0\n",
    "    return runtime_hours * hourly_price\n",
    "\n",
    "\n",
    "def calculate_cost_difference(tokens: float, runtime_seconds: float,\n",
    "                              api_price_per_1k: np.ndarray, hourly_price: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate cost difference (API - Local) for given tokens, runtime, and pricing grid.\n",
    "    \n",
    "    This function uses broadcasting to efficiently compute costs for all pricing combinations.\n",
    "    \n",
    "    Args:\n",
    "        tokens: Number of tokens (scalar)\n",
    "        runtime_seconds: Runtime in seconds (scalar)\n",
    "        api_price_per_1k: Array of API prices per 1k tokens (1D)\n",
    "        hourly_price: Array of hourly prices (1D)\n",
    "    \n",
    "    Returns:\n",
    "        2D array of cost differences (API - Local) for all pricing combinations\n",
    "        Shape: (len(hourly_price), len(api_price_per_1k))\n",
    "    \"\"\"\n",
    "    # Calculate API cost for all API prices\n",
    "    # Shape: (len(api_price_per_1k),)\n",
    "    api_costs = calculate_api_cost(tokens, api_price_per_1k)\n",
    "    \n",
    "    # Calculate local cost for all hourly prices\n",
    "    # Shape: (len(hourly_price),)\n",
    "    local_costs = calculate_local_cost(runtime_seconds, hourly_price)\n",
    "    \n",
    "    # Use broadcasting to create 2D grid\n",
    "    # api_costs[:, np.newaxis] creates shape (len(api_price_per_1k), 1)\n",
    "    # local_costs[np.newaxis, :] creates shape (1, len(hourly_price))\n",
    "    # Result: (len(hourly_price), len(api_price_per_1k))\n",
    "    api_cost_grid = api_costs[np.newaxis, :]  # Shape: (1, len(api_price_per_1k))\n",
    "    local_cost_grid = local_costs[:, np.newaxis]  # Shape: (len(hourly_price), 1)\n",
    "    \n",
    "    # Broadcast and compute difference\n",
    "    cost_difference = api_cost_grid - local_cost_grid\n",
    "    \n",
    "    return cost_difference\n",
    "\n",
    "\n",
    "# Test the functions with a simple example\n",
    "print(\"\ud83e\uddea Testing cost calculation functions:\")\n",
    "test_tokens = 10000.0\n",
    "test_runtime = 60.0  # 1 minute\n",
    "test_api_price = np.array([0.001, 0.005, 0.01])\n",
    "test_hourly_price = np.array([1.0, 2.0, 5.0])\n",
    "\n",
    "test_api_cost = calculate_api_cost(test_tokens, test_api_price)\n",
    "test_local_cost = calculate_local_cost(test_runtime, test_hourly_price)\n",
    "test_diff = calculate_cost_difference(test_tokens, test_runtime, test_api_price, test_hourly_price)\n",
    "\n",
    "print(f\"\\n   Test tokens: {test_tokens:,}\")\n",
    "print(f\"   Test runtime: {test_runtime} seconds ({test_runtime/3600:.4f} hours)\")\n",
    "print(f\"\\n   API costs for prices {test_api_price}:\")\n",
    "print(f\"      {test_api_cost}\")\n",
    "print(f\"\\n   Local costs for hourly rates {test_hourly_price}:\")\n",
    "print(f\"      {test_local_cost}\")\n",
    "print(f\"\\n   Cost difference grid (API - Local):\")\n",
    "print(f\"      Shape: {test_diff.shape}\")\n",
    "print(f\"      Values:\\n{test_diff}\")\n",
    "print(f\"\\n\u2705 Cost calculation functions working correctly!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Break-Even Analysis per Model\n",
    "\n",
    "Calculate costs for all pricing combinations for each model using individual experiment data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BREAK-EVEN ANALYSIS PER MODEL\n",
    "# ============================================================================\n",
    "# For each model combination, calculate costs across the entire pricing grid\n",
    "# using individual experiment data points\n",
    "\n",
    "# Storage for results\n",
    "break_even_results = {}\n",
    "model_analysis_results = {}\n",
    "\n",
    "# Group experiments by model combination\n",
    "model_groups = experiment_df.groupby(['gen_model', 'embed_model'])\n",
    "\n",
    "print(\"\ud83d\udd2c Starting break-even analysis for each model combination...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for (gen_model, embed_model), group_df in model_groups:\n",
    "    model_key = f\"{gen_model} + {embed_model}\"\n",
    "    print(f\"\\n\ud83d\udcca Analyzing: {model_key}\")\n",
    "    print(f\"   Experiments: {len(group_df)}\")\n",
    "    \n",
    "    # Get individual data points\n",
    "    tokens_list = group_df['total_tokens'].values\n",
    "    runtime_list = group_df['total_runtime_seconds'].values\n",
    "    \n",
    "    print(f\"   Tokens range: {tokens_list.min():,.0f} to {tokens_list.max():,.0f}\")\n",
    "    print(f\"   Runtime range: {runtime_list.min():.2f}s to {runtime_list.max():.2f}s\")\n",
    "    \n",
    "    # Calculate cost differences for each experiment across the pricing grid\n",
    "    # We'll average across experiments to get representative break-even points\n",
    "    all_cost_differences = []\n",
    "    \n",
    "    for i, (tokens, runtime) in enumerate(zip(tokens_list, runtime_list)):\n",
    "        # Calculate cost difference grid for this experiment\n",
    "        cost_diff = calculate_cost_difference(tokens, runtime, api_prices, hourly_prices)\n",
    "        all_cost_differences.append(cost_diff)\n",
    "        \n",
    "        if (i + 1) % 5 == 0 or (i + 1) == len(tokens_list):\n",
    "            print(f\"      Processed {i + 1}/{len(tokens_list)} experiments...\", end='\\r')\n",
    "    \n",
    "    print()  # New line after progress\n",
    "    \n",
    "    # Average cost differences across all experiments for this model\n",
    "    # This gives us the expected cost difference for this model at each pricing point\n",
    "    avg_cost_difference = np.mean(all_cost_differences, axis=0)\n",
    "    \n",
    "    # Store results\n",
    "    model_analysis_results[model_key] = {\n",
    "        'gen_model': gen_model,\n",
    "        'embed_model': embed_model,\n",
    "        'num_experiments': len(group_df),\n",
    "        'tokens_list': tokens_list,\n",
    "        'runtime_list': runtime_list,\n",
    "        'avg_cost_difference': avg_cost_difference,\n",
    "        'all_cost_differences': all_cost_differences  # Store individual for variance analysis\n",
    "    }\n",
    "    \n",
    "    print(f\"   \u2705 Analysis complete for {model_key}\")\n",
    "\n",
    "print(f\"\\n\u2705 Break-even analysis complete for {len(model_analysis_results)} model combinations!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Break-Even Point Detection\n",
    "\n",
    "Identify where API cost equals Local cost (break-even points) for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BREAK-EVEN POINT DETECTION\n",
    "# ============================================================================\n",
    "# Find where cost_difference \u2248 0 (within tolerance)\n",
    "\n",
    "BREAK_EVEN_TOLERANCE = 0.0001  # Tolerance for break-even detection ($0.0001)\n",
    "\n",
    "print(\"\ud83c\udfaf Detecting break-even points...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_key, results in model_analysis_results.items():\n",
    "    print(f\"\\n\ud83d\udcca Finding break-even points for: {model_key}\")\n",
    "    \n",
    "    avg_cost_diff = results['avg_cost_difference']\n",
    "    \n",
    "    # Find where cost difference is approximately zero\n",
    "    # abs(cost_difference) < tolerance\n",
    "    break_even_mask = np.abs(avg_cost_diff) < BREAK_EVEN_TOLERANCE\n",
    "    \n",
    "    # Extract break-even points\n",
    "    break_even_points = []\n",
    "    \n",
    "    # For each hourly price, find corresponding API prices where break-even occurs\n",
    "    for hourly_idx, hourly_price in enumerate(hourly_prices):\n",
    "        # Get cost differences for this hourly price across all API prices\n",
    "        cost_diff_row = avg_cost_diff[hourly_idx, :]\n",
    "        \n",
    "        # Find API prices where break-even occurs\n",
    "        break_even_indices = np.where(np.abs(cost_diff_row) < BREAK_EVEN_TOLERANCE)[0]\n",
    "        \n",
    "        for api_idx in break_even_indices:\n",
    "            api_price = api_prices[api_idx]\n",
    "            cost_diff = cost_diff_row[api_idx]\n",
    "            \n",
    "            break_even_points.append({\n",
    "                'gen_model': results['gen_model'],\n",
    "                'embed_model': results['embed_model'],\n",
    "                'api_price_per_1k': api_price,\n",
    "                'hourly_price': hourly_price,\n",
    "                'cost_difference': cost_diff,\n",
    "                'api_cost': calculate_api_cost(np.mean(results['tokens_list']), np.array([api_price]))[0],\n",
    "                'local_cost': calculate_local_cost(np.mean(results['runtime_list']), np.array([hourly_price]))[0]\n",
    "            })\n",
    "    \n",
    "    # Also find break-even curves by interpolating\n",
    "    # For each hourly price, find the API price where cost difference = 0\n",
    "    break_even_curves = []\n",
    "    \n",
    "    for hourly_idx, hourly_price in enumerate(hourly_prices):\n",
    "        cost_diff_row = avg_cost_diff[hourly_idx, :]\n",
    "        \n",
    "        # Find where cost difference crosses zero\n",
    "        # Look for sign changes\n",
    "        sign_changes = np.where(np.diff(np.sign(cost_diff_row)))[0]\n",
    "        \n",
    "        for sign_change_idx in sign_changes:\n",
    "            # Interpolate to find exact break-even point\n",
    "            x0, x1 = api_prices[sign_change_idx], api_prices[sign_change_idx + 1]\n",
    "            y0, y1 = cost_diff_row[sign_change_idx], cost_diff_row[sign_change_idx + 1]\n",
    "            \n",
    "            # Linear interpolation to find where y = 0\n",
    "            if y0 != y1:  # Avoid division by zero\n",
    "                api_price_break_even = x0 - y0 * (x1 - x0) / (y1 - y0)\n",
    "                \n",
    "                # Verify it's in valid range\n",
    "                if api_price_min <= api_price_break_even <= api_price_max:\n",
    "                    break_even_curves.append({\n",
    "                        'gen_model': results['gen_model'],\n",
    "                        'embed_model': results['embed_model'],\n",
    "                        'api_price_per_1k': api_price_break_even,\n",
    "                        'hourly_price': hourly_price,\n",
    "                        'cost_difference': 0.0\n",
    "                    })\n",
    "    \n",
    "    # Store break-even points\n",
    "    break_even_results[model_key] = {\n",
    "        'break_even_points': break_even_points,\n",
    "        'break_even_curves': break_even_curves,\n",
    "        'num_break_even_points': len(break_even_points),\n",
    "        'num_break_even_curve_points': len(break_even_curves)\n",
    "    }\n",
    "    \n",
    "    print(f\"   Found {len(break_even_points)} break-even points\")\n",
    "    print(f\"   Found {len(break_even_curves)} break-even curve points\")\n",
    "\n",
    "print(f\"\\n\u2705 Break-even detection complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Visualization\n",
    "\n",
    "Create comprehensive visualizations of break-even points and cost differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION: Break-Even Curves\n",
    "# ============================================================================\n",
    "# Plot break-even API price vs hourly rate for each model\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Break-Even Curves: API Price vs Hourly Rate', fontsize=16, fontweight='bold')\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "model_keys = list(model_analysis_results.keys())\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(model_keys)))\n",
    "\n",
    "for idx, (model_key, results) in enumerate(model_analysis_results.items()):\n",
    "    if idx >= len(axes_flat):\n",
    "        break\n",
    "    \n",
    "    ax = axes_flat[idx]\n",
    "    break_even_data = break_even_results[model_key]\n",
    "    \n",
    "    # Extract break-even curve points\n",
    "    if break_even_data['break_even_curves']:\n",
    "        curve_df = pd.DataFrame(break_even_data['break_even_curves'])\n",
    "        \n",
    "        # Sort by hourly price for smooth curve\n",
    "        curve_df = curve_df.sort_values('hourly_price')\n",
    "        \n",
    "        # Plot break-even curve\n",
    "        ax.plot(curve_df['hourly_price'], curve_df['api_price_per_1k'], \n",
    "               'o-', label='Break-even', linewidth=2, markersize=4, color=colors[idx])\n",
    "        \n",
    "        # Fill area: above curve = API cheaper, below = Local cheaper\n",
    "        ax.fill_between(curve_df['hourly_price'], curve_df['api_price_per_1k'], \n",
    "                       api_price_max, alpha=0.2, color='green', label='API Cheaper')\n",
    "        ax.fill_between(curve_df['hourly_price'], api_price_min, curve_df['api_price_per_1k'], \n",
    "                       alpha=0.2, color='red', label='Local Cheaper')\n",
    "    \n",
    "    ax.set_xlabel('Local Hourly Price ($/hour)', fontsize=11)\n",
    "    ax.set_ylabel('API Price per 1k Tokens ($)', fontsize=11)\n",
    "    ax.set_title(f'{model_key}', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.set_xlim(hourly_price_min, hourly_price_max)\n",
    "    ax.set_ylim(api_price_min, api_price_max)\n",
    "\n",
    "plt.tight_layout()\n",
    "break_even_curves_path = ANALYSIS_DIR / f\"break_even_curves_{analysis_timestamp}.png\"\n",
    "plt.savefig(break_even_curves_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"\u2705 Saved break-even curves: {break_even_curves_path.name}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION: Cost Difference Heatmaps\n",
    "# ============================================================================\n",
    "# Show cost difference (API - Local) as heatmaps for each model\n",
    "\n",
    "# Select a few representative models for heatmaps (to avoid too many plots)\n",
    "selected_models = list(model_analysis_results.keys())[:4]  # First 4 models\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Cost Difference Heatmaps (API - Local)', fontsize=16, fontweight='bold')\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, model_key in enumerate(selected_models):\n",
    "    if idx >= len(axes_flat):\n",
    "        break\n",
    "    \n",
    "    ax = axes_flat[idx]\n",
    "    results = model_analysis_results[model_key]\n",
    "    avg_cost_diff = results['avg_cost_difference']\n",
    "    \n",
    "    # Create heatmap\n",
    "    # Note: We'll sample the grid for visualization (every 10th point) to make it manageable\n",
    "    sample_rate = 10\n",
    "    api_prices_sampled = api_prices[::sample_rate]\n",
    "    hourly_prices_sampled = hourly_prices[::sample_rate]\n",
    "    cost_diff_sampled = avg_cost_diff[::sample_rate, ::sample_rate]\n",
    "    \n",
    "    im = ax.contourf(api_prices_sampled, hourly_prices_sampled, cost_diff_sampled, \n",
    "                    levels=20, cmap='RdBu_r', extend='both')\n",
    "    \n",
    "    # Add break-even line (where cost difference = 0)\n",
    "    contour = ax.contour(api_prices_sampled, hourly_prices_sampled, cost_diff_sampled, \n",
    "                        levels=[0], colors='black', linewidths=2, linestyles='--')\n",
    "    ax.clabel(contour, inline=True, fontsize=8, fmt='Break-even')\n",
    "    \n",
    "    ax.set_xlabel('API Price per 1k Tokens ($)', fontsize=11)\n",
    "    ax.set_ylabel('Local Hourly Price ($/hour)', fontsize=11)\n",
    "    ax.set_title(f'{model_key}', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Cost Difference: API - Local ($)', fontsize=10)\n",
    "    \n",
    "    # Add text annotation\n",
    "    ax.text(0.02, 0.98, f'Red = Local Cheaper\\nBlue = API Cheaper', \n",
    "           transform=ax.transAxes, fontsize=9, verticalalignment='top',\n",
    "           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "heatmaps_path = ANALYSIS_DIR / f\"cost_difference_heatmaps_{analysis_timestamp}.png\"\n",
    "plt.savefig(heatmaps_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"\u2705 Saved heatmaps: {heatmaps_path.name}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION: Model Comparison - Break-Even Points\n",
    "# ============================================================================\n",
    "# Compare break-even points across all models on a single plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(model_analysis_results)))\n",
    "\n",
    "for idx, (model_key, results) in enumerate(model_analysis_results.items()):\n",
    "    break_even_data = break_even_results[model_key]\n",
    "    \n",
    "    if break_even_data['break_even_curves']:\n",
    "        curve_df = pd.DataFrame(break_even_data['break_even_curves'])\n",
    "        curve_df = curve_df.sort_values('hourly_price')\n",
    "        \n",
    "        # Plot break-even curve for this model\n",
    "        ax.plot(curve_df['hourly_price'], curve_df['api_price_per_1k'], \n",
    "               'o-', label=model_key, linewidth=2, markersize=3, color=colors[idx], alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Local Hourly Price ($/hour)', fontsize=12)\n",
    "ax.set_ylabel('API Price per 1k Tokens ($)', fontsize=12)\n",
    "ax.set_title('Break-Even Comparison: All Models', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "ax.set_xlim(hourly_price_min, hourly_price_max)\n",
    "ax.set_ylim(api_price_min, api_price_max)\n",
    "\n",
    "# Add reference lines for common pricing points\n",
    "common_hourly = [0.5, 1.0, 2.0, 5.0]\n",
    "for h in common_hourly:\n",
    "    ax.axvline(x=h, color='gray', linestyle=':', alpha=0.5, linewidth=1)\n",
    "\n",
    "common_api = [0.001, 0.005, 0.01]\n",
    "for a in common_api:\n",
    "    ax.axhline(y=a, color='gray', linestyle=':', alpha=0.5, linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "comparison_path = ANALYSIS_DIR / f\"break_even_comparison_{analysis_timestamp}.png\"\n",
    "plt.savefig(comparison_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"\u2705 Saved model comparison: {comparison_path.name}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Break-Even Summary Tables\n",
    "\n",
    "Create summary tables showing break-even points for common pricing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BREAK-EVEN SUMMARY TABLES\n",
    "# ============================================================================\n",
    "\n",
    "# Table 1: Break-even API price for common hourly rates\n",
    "common_hourly_rates = [0.5, 1.0, 1.5, 2.0, 3.0, 4.0, 5.0]\n",
    "\n",
    "summary_data = []\n",
    "for model_key, results in model_analysis_results.items():\n",
    "    break_even_data = break_even_results[model_key]\n",
    "    \n",
    "    if break_even_data['break_even_curves']:\n",
    "        curve_df = pd.DataFrame(break_even_data['break_even_curves'])\n",
    "        \n",
    "        row = {\n",
    "            'gen_model': results['gen_model'],\n",
    "            'embed_model': results['embed_model'],\n",
    "            'model_combination': model_key\n",
    "        }\n",
    "        \n",
    "        # For each common hourly rate, find closest break-even API price\n",
    "        for hourly_rate in common_hourly_rates:\n",
    "            # Find closest hourly price in break-even curve\n",
    "            closest_idx = np.argmin(np.abs(curve_df['hourly_price'] - hourly_rate))\n",
    "            closest_hourly = curve_df.iloc[closest_idx]['hourly_price']\n",
    "            \n",
    "            # If close enough, use that API price; otherwise interpolate\n",
    "            if abs(closest_hourly - hourly_rate) < 0.1:\n",
    "                api_price = curve_df.iloc[closest_idx]['api_price_per_1k']\n",
    "            else:\n",
    "                # Interpolate\n",
    "                if len(curve_df) > 1:\n",
    "                    interp_func = interpolate.interp1d(curve_df['hourly_price'], \n",
    "                                                      curve_df['api_price_per_1k'],\n",
    "                                                      kind='linear', \n",
    "                                                      bounds_error=False, \n",
    "                                                      fill_value=np.nan)\n",
    "                    api_price = interp_func(hourly_rate)\n",
    "                else:\n",
    "                    api_price = np.nan\n",
    "            \n",
    "            row[f'api_price_at_${hourly_rate}_per_hour'] = api_price if not np.isnan(api_price) else None\n",
    "        \n",
    "        summary_data.append(row)\n",
    "\n",
    "break_even_summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\ud83d\udcca Break-Even Summary: API Price at Common Hourly Rates\")\n",
    "print(\"=\" * 100)\n",
    "print(break_even_summary_df.to_string(index=False))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2: Break-even hourly rate for common API prices\n",
    "common_api_prices = [0.0005, 0.001, 0.002, 0.005, 0.01]\n",
    "\n",
    "summary_data2 = []\n",
    "for model_key, results in model_analysis_results.items():\n",
    "    break_even_data = break_even_results[model_key]\n",
    "    \n",
    "    if break_even_data['break_even_curves']:\n",
    "        curve_df = pd.DataFrame(break_even_data['break_even_curves'])\n",
    "        curve_df = curve_df.sort_values('api_price_per_1k')\n",
    "        \n",
    "        row = {\n",
    "            'gen_model': results['gen_model'],\n",
    "            'embed_model': results['embed_model'],\n",
    "            'model_combination': model_key\n",
    "        }\n",
    "        \n",
    "        # For each common API price, find closest break-even hourly rate\n",
    "        for api_price in common_api_prices:\n",
    "            # Find closest API price in break-even curve\n",
    "            closest_idx = np.argmin(np.abs(curve_df['api_price_per_1k'] - api_price))\n",
    "            closest_api = curve_df.iloc[closest_idx]['api_price_per_1k']\n",
    "            \n",
    "            # If close enough, use that hourly rate; otherwise interpolate\n",
    "            if abs(closest_api - api_price) < 0.0005:\n",
    "                hourly_rate = curve_df.iloc[closest_idx]['hourly_price']\n",
    "            else:\n",
    "                # Interpolate\n",
    "                if len(curve_df) > 1:\n",
    "                    interp_func = interpolate.interp1d(curve_df['api_price_per_1k'], \n",
    "                                                      curve_df['hourly_price'],\n",
    "                                                      kind='linear', \n",
    "                                                      bounds_error=False, \n",
    "                                                      fill_value=np.nan)\n",
    "                    hourly_rate = interp_func(api_price)\n",
    "                else:\n",
    "                    hourly_rate = np.nan\n",
    "            \n",
    "            row[f'hourly_rate_at_${api_price}_per_1k_tokens'] = hourly_rate if not np.isnan(hourly_rate) else None\n",
    "        \n",
    "        summary_data2.append(row)\n",
    "\n",
    "break_even_summary2_df = pd.DataFrame(summary_data2)\n",
    "\n",
    "print(\"\ud83d\udcca Break-Even Summary: Hourly Rate at Common API Prices\")\n",
    "print(\"=\" * 100)\n",
    "print(break_even_summary2_df.to_string(index=False))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Data Export\n",
    "\n",
    "Export all break-even analysis results to CSV files with timestamps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA EXPORT\n",
    "# ============================================================================\n",
    "\n",
    "export_timestamp = datetime.now().isoformat()\n",
    "export_timestamp_readable = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "export_timestamp_filename = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(\"\ud83d\udcbe Exporting break-even analysis results...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Export all break-even points\n",
    "all_break_even_points = []\n",
    "for model_key, break_even_data in break_even_results.items():\n",
    "    if break_even_data['break_even_points']:\n",
    "        for point in break_even_data['break_even_points']:\n",
    "            point['export_timestamp'] = export_timestamp\n",
    "            point['export_timestamp_readable'] = export_timestamp_readable\n",
    "            all_break_even_points.append(point)\n",
    "\n",
    "if all_break_even_points:\n",
    "    break_even_points_df = pd.DataFrame(all_break_even_points)\n",
    "    break_even_points_path = ANALYSIS_DIR / f\"break_even_points_{export_timestamp_filename}.csv\"\n",
    "    break_even_points_df.to_csv(break_even_points_path, index=False)\n",
    "    print(f\"\u2705 1. Break-Even Points\")\n",
    "    print(f\"   File: {break_even_points_path.name}\")\n",
    "    print(f\"   Rows: {len(break_even_points_df)}\")\n",
    "    print()\n",
    "\n",
    "# 2. Export break-even curves\n",
    "all_break_even_curves = []\n",
    "for model_key, break_even_data in break_even_results.items():\n",
    "    if break_even_data['break_even_curves']:\n",
    "        for point in break_even_data['break_even_curves']:\n",
    "            point['export_timestamp'] = export_timestamp\n",
    "            point['export_timestamp_readable'] = export_timestamp_readable\n",
    "            all_break_even_curves.append(point)\n",
    "\n",
    "if all_break_even_curves:\n",
    "    break_even_curves_df = pd.DataFrame(all_break_even_curves)\n",
    "    break_even_curves_path = ANALYSIS_DIR / f\"break_even_curves_{export_timestamp_filename}.csv\"\n",
    "    break_even_curves_df.to_csv(break_even_curves_path, index=False)\n",
    "    print(f\"\u2705 2. Break-Even Curves\")\n",
    "    print(f\"   File: {break_even_curves_path.name}\")\n",
    "    print(f\"   Rows: {len(break_even_curves_df)}\")\n",
    "    print()\n",
    "\n",
    "# 3. Export summary tables\n",
    "if not break_even_summary_df.empty:\n",
    "    break_even_summary_df['export_timestamp'] = export_timestamp\n",
    "    break_even_summary_df['export_timestamp_readable'] = export_timestamp_readable\n",
    "    cols = ['export_timestamp', 'export_timestamp_readable'] + [c for c in break_even_summary_df.columns if c not in ['export_timestamp', 'export_timestamp_readable']]\n",
    "    break_even_summary_df = break_even_summary_df[cols]\n",
    "    \n",
    "    summary_path = ANALYSIS_DIR / f\"break_even_summary_api_prices_{export_timestamp_filename}.csv\"\n",
    "    break_even_summary_df.to_csv(summary_path, index=False)\n",
    "    print(f\"\u2705 3. Break-Even Summary (API Prices)\")\n",
    "    print(f\"   File: {summary_path.name}\")\n",
    "    print(f\"   Rows: {len(break_even_summary_df)}\")\n",
    "    print()\n",
    "\n",
    "if not break_even_summary2_df.empty:\n",
    "    break_even_summary2_df['export_timestamp'] = export_timestamp\n",
    "    break_even_summary2_df['export_timestamp_readable'] = export_timestamp_readable\n",
    "    cols = ['export_timestamp', 'export_timestamp_readable'] + [c for c in break_even_summary2_df.columns if c not in ['export_timestamp', 'export_timestamp_readable']]\n",
    "    break_even_summary2_df = break_even_summary2_df[cols]\n",
    "    \n",
    "    summary2_path = ANALYSIS_DIR / f\"break_even_summary_hourly_rates_{export_timestamp_filename}.csv\"\n",
    "    break_even_summary2_df.to_csv(summary2_path, index=False)\n",
    "    print(f\"\u2705 4. Break-Even Summary (Hourly Rates)\")\n",
    "    print(f\"   File: {summary2_path.name}\")\n",
    "    print(f\"   Rows: {len(break_even_summary2_df)}\")\n",
    "    print()\n",
    "\n",
    "print(f\"\ud83d\udcc1 All files saved to: {ANALYSIS_DIR}\")\n",
    "print(f\"\u2705 Export complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}