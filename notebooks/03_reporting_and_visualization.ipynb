{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 03: Reporting and Visualization\n",
        "\n",
        "This notebook creates a comprehensive report summarizing all metrics collected during embedding and inference operations.\n",
        "\n",
        "## Report Contents\n",
        "\n",
        "1. **Executive Summary**: High-level overview of the experiment\n",
        "2. **Metrics Dashboard**: Detailed tables with token and timing statistics\n",
        "3. **Visualizations**: Charts showing distributions, relationships, and comparisons\n",
        "4. **Interpretation**: Analysis of what the metrics mean and practical implications\n",
        "\n",
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import standard library modules\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add the src directory to Python path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Import our custom modules\n",
        "from src.config import Config\n",
        "from src.reporting import (\n",
        "    load_metrics,\n",
        "    aggregate_metrics,\n",
        "    create_summary_tables,\n",
        "    plot_token_distribution,\n",
        "    plot_latency_vs_tokens,\n",
        "    plot_throughput_comparison,\n",
        "    plot_latency_distribution\n",
        ")\n",
        "\n",
        "# Import visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Set up matplotlib for better-looking plots\n",
        "# Try to use a nice style, fall back to default if not available\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "except OSError:\n",
        "    try:\n",
        "        plt.style.use('seaborn-darkgrid')\n",
        "    except OSError:\n",
        "        plt.style.use('default')\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"Modules imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Metrics\n",
        "\n",
        "Load the metrics collected from notebooks 01 and 02.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create configuration to get paths\n",
        "config = Config()\n",
        "\n",
        "# Load metrics from JSON file\n",
        "metrics_path = config.get_metrics_path()\n",
        "print(f\"Loading metrics from: {metrics_path}\")\n",
        "\n",
        "# Load into pandas DataFrame for easy analysis\n",
        "df = load_metrics(metrics_path)\n",
        "\n",
        "print(f\"Loaded {len(df)} metric records\")\n",
        "print(f\"\\nColumns: {list(df.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Executive Summary\n",
        "\n",
        "High-level overview of the experiment and key findings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment Overview\n",
        "\n",
        "This experiment demonstrates token and timing tracking for LLM operations using:\n",
        "- **Embedding Model**: Gemma embedding model via Ollama (`embeddinggemma`)\n",
        "- **Generation Model**: Gemma3 1B model via Ollama (`gemma3:1b`)\n",
        "- **Infrastructure**: Local Ollama instance running on `http://localhost:11434`\n",
        "\n",
        "The workflow involved:\n",
        "1. Loading and chunking a text document\n",
        "2. Embedding all chunks to create vector representations\n",
        "3. Generating questions and performing inference operations\n",
        "4. Tracking all tokens and timing metrics throughout\n",
        "\n",
        "### Key Objectives\n",
        "\n",
        "- Measure token usage for embedding vs inference operations\n",
        "- Track latency (time per call) for different operation types\n",
        "- Calculate throughput (tokens per second)\n",
        "- Understand performance characteristics of Gemma models via Ollama\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics Dashboard\n",
        "\n",
        "Aggregate all metrics and display summary statistics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate metrics to compute summary statistics\n",
        "aggregated = aggregate_metrics(df)\n",
        "\n",
        "# Create summary table\n",
        "summary_table = create_summary_tables(aggregated)\n",
        "\n",
        "print(\"Summary Statistics\")\n",
        "print(\"=\" * 80)\n",
        "# Display the table (display() works in Jupyter, print() works everywhere)\n",
        "try:\n",
        "    display(summary_table)  # Jupyter/IPython\n",
        "except NameError:\n",
        "    print(summary_table)  # Fallback for non-Jupyter environments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Breakdown\n",
        "\n",
        "Let's examine the aggregated metrics in more detail:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display detailed metrics\n",
        "if 'embedding' in aggregated:\n",
        "    print(\"Embedding Metrics:\")\n",
        "    print(\"-\" * 40)\n",
        "    emb = aggregated['embedding']\n",
        "    for key, value in emb.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"  {key}: {value:.2f}\")\n",
        "        else:\n",
        "            print(f\"  {key}: {value}\")\n",
        "    print()\n",
        "\n",
        "if 'inference' in aggregated:\n",
        "    print(\"Inference Metrics:\")\n",
        "    print(\"-\" * 40)\n",
        "    inf = aggregated['inference']\n",
        "    for key, value in inf.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"  {key}: {value:.2f}\")\n",
        "        else:\n",
        "            print(f\"  {key}: {value}\")\n",
        "    print()\n",
        "\n",
        "if 'overall' in aggregated:\n",
        "    print(\"Overall Metrics:\")\n",
        "    print(\"-\" * 40)\n",
        "    ovr = aggregated['overall']\n",
        "    for key, value in ovr.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"  {key}: {value:.2f}\")\n",
        "        else:\n",
        "            print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations\n",
        "\n",
        "Create visualizations to better understand the data distributions and relationships.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Token Distribution (for Chunks)\n",
        "\n",
        "This histogram shows how tokens are distributed across chunks. This helps understand if chunking is consistent or varies significantly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load chunks to get token distribution\n",
        "import json\n",
        "\n",
        "chunks_path = config.get_chunks_path()\n",
        "if chunks_path.exists():\n",
        "    with open(chunks_path, 'r') as f:\n",
        "        chunks = json.load(f)\n",
        "    \n",
        "    # Create DataFrame from chunks\n",
        "    chunks_df = pd.DataFrame(chunks)\n",
        "    \n",
        "    # Plot token distribution\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    plot_token_distribution(chunks_df, ax)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Chunks file not found. Run notebook 01 first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Latency vs Token Count\n",
        "\n",
        "This scatter plot shows the relationship between the number of tokens and the time it takes to process them. Generally, more tokens should take longer, but the relationship may not be perfectly linear.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot latency vs tokens\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plot_latency_vs_tokens(df, ax)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Throughput Comparison\n",
        "\n",
        "This bar chart compares tokens per second (throughput) between embedding and inference operations. Higher is better - it means the system processes tokens faster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot throughput comparison\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plot_throughput_comparison(aggregated, ax)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Latency Distribution\n",
        "\n",
        "This histogram shows the distribution of call latencies. A narrow distribution means consistent performance, while a wide distribution indicates variability in call times.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot latency distribution\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plot_latency_distribution(df, ax)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpretation and Analysis\n",
        "\n",
        "### What Do These Numbers Mean?\n",
        "\n",
        "**Token Usage:**\n",
        "- Tokens are the basic units that language models process. One token is roughly 0.75 words in English.\n",
        "- Total tokens show the overall \"cost\" of operations in terms of processing units.\n",
        "- Comparing embedding vs inference tokens helps understand where most processing happens.\n",
        "\n",
        "**Latency:**\n",
        "- Latency is the time it takes for a single operation to complete.\n",
        "- Average latency gives you the typical wait time per call.\n",
        "- P95 latency (95th percentile) shows the worst-case scenario for 95% of calls - useful for understanding tail performance.\n",
        "\n",
        "**Throughput:**\n",
        "- Tokens per second measures how fast the system processes tokens.\n",
        "- Higher throughput means the system is more efficient.\n",
        "- Comparing embedding vs inference throughput shows which operation type is faster.\n",
        "\n",
        "### Key Insights\n",
        "\n",
        "1. **Embedding vs Inference Cost**: \n",
        "   - Embedding operations typically process more tokens (the entire document), but may be faster per token.\n",
        "   - Inference operations process fewer tokens per call but may be slower due to generation complexity.\n",
        "\n",
        "2. **Performance Characteristics**:\n",
        "   - If latency varies significantly (wide distribution), it may indicate system load or model variability.\n",
        "   - Consistent latency suggests stable performance.\n",
        "\n",
        "3. **Practical Implications**:\n",
        "   - For large documents, embedding time can be significant - consider batch processing or parallelization.\n",
        "   - Inference latency directly impacts user experience - optimize prompts to reduce token counts when possible.\n",
        "   - Throughput metrics help estimate total processing time for large workloads.\n",
        "\n",
        "### Recommendations\n",
        "\n",
        "- **For Production Use**: Monitor P95 latency to ensure acceptable worst-case performance.\n",
        "- **For Cost Optimization**: Track total tokens to estimate computational costs.\n",
        "- **For Performance Tuning**: Use throughput metrics to identify bottlenecks and optimization opportunities.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
