{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOLH Experimental Design for Cost Sensitivity Analysis\n",
    "\n",
    "This notebook uses **Nearly Orthogonal Latin Hypercube (NOLH)** sampling to efficiently explore the parameter space with fewer experiments than a full factorial design.\n",
    "\n",
    "## Overview\n",
    "\n",
    "NOLH sampling is a space-filling design that:\n",
    "- Provides good coverage of the parameter space with fewer samples\n",
    "- Ensures samples are well-distributed across all dimensions\n",
    "- Is more efficient than full factorial designs for high-dimensional spaces\n",
    "\n",
    "## Parameters Being Explored\n",
    "\n",
    "- **num_docs**: 5-50 (number of document chunks to process)\n",
    "- **chunk_size**: 256-4000 (tokens per chunk)\n",
    "- **num_questions_per_doc**: 1-10 (questions generated per chunk)\n",
    "- **gen_model**: Generation models (manually specified in Section 2)\n",
    "- **embed_model**: Embedding models (manually specified in Section 2)\n",
    "\n",
    "## How to Use\n",
    "\n",
    "1. **Configure Parameters**: Adjust parameter ranges and number of samples in Section 2\n",
    "2. **Generate Samples**: Execute Section 3 to generate NOLH sample points\n",
    "3. **Run Experiments**: Execute Section 6 to run all experiments\n",
    "4. **Analyze Results**: Review tables and visualizations in Sections 7-9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import standard library modules\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "\n",
    "# Add the src directory to Python path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import our custom modules\n",
    "from src.config import Config\n",
    "from src.pipeline import (\n",
    "    load_document, \n",
    "    chunk_text, \n",
    "    embed_chunks, \n",
    "    generate_questions,\n",
    "    save_chunks,\n",
    "    save_metrics\n",
    ")\n",
    "from src.timing_metrics import MetricsStore\n",
    "from src.reporting import aggregate_metrics, load_metrics\n",
    "from src.token_accounting import count_tokens\n",
    "\n",
    "# Import data analysis and visualization libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # For 3D plots\n",
    "\n",
    "# Import scipy for Latin Hypercube sampling\n",
    "from scipy.stats import qmc\n",
    "\n",
    "# Set up matplotlib for better-looking plots\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except OSError:\n",
    "    try:\n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "    except OSError:\n",
    "        plt.style.use('default')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Modules imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.5: Optional - Discover Available Ollama Models\n",
    "\n",
    "This section helps you see what models are available in your Ollama installation. This is optional - you can manually specify models in Section 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 11 available models in Ollama:\n",
      "  1. embeddinggemma:latest\n",
      "  2. gemma3:12b\n",
      "  3. gemma3:1b\n",
      "  4. gemma3:27b\n",
      "  5. gemma3:4b\n",
      "  6. gpt-oss:120b\n",
      "  7. gpt-oss:20b\n",
      "  8. nomic-embed-text:137m-v1.5-fp16\n",
      "  9. qwen3-embedding:8b-fp16\n",
      "  10. qwen3:latest\n",
      "  11. snowflake-arctic-embed2:568m\n",
      "\n",
      "ðŸ’¡ You can use these model names in Section 2 when specifying generation_models and embedding_models\n"
     ]
    }
   ],
   "source": [
    "# Optional helper function to discover available Ollama models\n",
    "# This helps you see what models you have installed before manually specifying them\n",
    "import requests\n",
    "\n",
    "def discover_ollama_models(base_url: str = \"http://localhost:11434\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Discover available models from Ollama API.\n",
    "    \n",
    "    This function queries the Ollama API to get a list of all available models.\n",
    "    It's a helper function to see what models you have installed.\n",
    "    \n",
    "    Args:\n",
    "        base_url: The base URL where Ollama is running (default: localhost:11434)\n",
    "    \n",
    "    Returns:\n",
    "        List of model names (sorted alphabetically)\n",
    "    \n",
    "    Raises:\n",
    "        ConnectionError: If Ollama is not running or not accessible\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Query Ollama's /api/tags endpoint to get available models\n",
    "        # This endpoint lists all models without requiring any input\n",
    "        url = f\"{base_url}/api/tags\"\n",
    "        \n",
    "        # Make a GET request with a timeout\n",
    "        # GET is used because we're just asking for information, not sending data\n",
    "        response = requests.get(url, timeout=5)\n",
    "        \n",
    "        # Check if the request was successful (HTTP 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the JSON response\n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract model names from the response\n",
    "            # Ollama returns models in a 'models' list, each with a 'name' field\n",
    "            models = [model.get('name', '') for model in data.get('models', [])]\n",
    "            \n",
    "            # Sort models alphabetically for consistent ordering\n",
    "            models.sort()\n",
    "            \n",
    "            return models\n",
    "        else:\n",
    "            # Got a response but it's an error\n",
    "            raise ConnectionError(f\"Ollama API returned status {response.status_code}: {response.text}\")\n",
    "            \n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        # Connection refused or couldn't reach the server\n",
    "        raise ConnectionError(f\"Could not connect to Ollama at {base_url}. Make sure Ollama is running.\") from e\n",
    "    except requests.exceptions.Timeout:\n",
    "        # Request took too long\n",
    "        raise ConnectionError(f\"Connection to Ollama timed out. Check if Ollama is running at {base_url}.\")\n",
    "    except Exception as e:\n",
    "        # Some other error\n",
    "        raise ConnectionError(f\"Error discovering models: {str(e)}\") from e\n",
    "\n",
    "# Try to discover models (optional - will show error if Ollama not running, but that's OK)\n",
    "try:\n",
    "    available_models = discover_ollama_models()\n",
    "    print(f\"âœ… Found {len(available_models)} available models in Ollama:\")\n",
    "    for i, model in enumerate(available_models, 1):\n",
    "        print(f\"  {i}. {model}\")\n",
    "    print(\"\\nðŸ’¡ You can use these model names in Section 2 when specifying generation_models and embedding_models\")\n",
    "except ConnectionError as e:\n",
    "    print(f\"âš ï¸  Could not discover models: {e}\")\n",
    "    print(\"   This is OK - you can still manually specify models in Section 2\")\n",
    "    print(\"   Make sure Ollama is running if you want to use the discovery feature\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results directory: results\n",
      "NOLH experiments directory: results/experiments/nolh\n",
      "Document path: /home/goble54/spark-dev-workspace/GenerativeAI-Cost-Estimator/MobyDick.txt\n"
     ]
    }
   ],
   "source": [
    "# Set up paths\n",
    "base_config = Config()\n",
    "experiments_dir = base_config.results_dir / \"experiments\" / \"nolh\"\n",
    "experiments_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Results directory: {base_config.results_dir}\")\n",
    "print(f\"NOLH experiments directory: {experiments_dir}\")\n",
    "\n",
    "# Document path (using MobyDick.txt as example)\n",
    "document_path = project_root / \"MobyDick.txt\"\n",
    "print(f\"Document path: {document_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: NOLH Parameter Configuration\n",
    "\n",
    "Define the parameter ranges and model configurations for all experiments.\n",
    "\n",
    "**IMPORTANT**: You must manually specify which models to use for generation and embedding below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model configuration:\n",
      "  Generation models (7): ['gemma3:1b', 'gemma3:4b', 'gemma3:12b', 'gemma3:27b', 'gpt-oss:20b', 'qwen3:latest', 'gpt-oss:120b']\n",
      "  Embedding models (4): ['embeddinggemma', 'qwen3-embedding:8b-fp16', 'snowflake-arctic-embed2:568m', 'nomic-embed-text:137m-v1.5-fp16']\n",
      "  Total model combinations: 28\n",
      "\n",
      "âœ… Parameter configuration:\n",
      "  Parameter ranges: {'num_docs': (5, 50), 'chunk_size': (256, 4000), 'num_questions_per_doc': (1, 10), 'gen_model_idx': (0, 6), 'embed_model_idx': (0, 3)}\n",
      "  Number of samples: 30\n",
      "  Fixed config: {'concurrency': 3}\n",
      "  API pricing: {'mode': 'api', 'price_per_1k_tokens_input': 0.0025, 'price_per_1k_tokens_output': 0.01}\n",
      "  Local pricing: {'mode': 'local', 'dollars_per_gpu_hour': 1.5}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# USER INPUT: Specify Models to Use\n",
    "# ============================================================================\n",
    "# IMPORTANT: Edit these lists to specify which models you want to test\n",
    "# \n",
    "# Examples:\n",
    "#   generation_models = ['gemma3:1b', 'gemma3:4b']\n",
    "#   embedding_models = ['embeddinggemma']\n",
    "#\n",
    "# You can see available models by running Section 1.5 above (optional)\n",
    "\n",
    "# List of generation models to test\n",
    "# These models will be used for text generation (question generation)\n",
    "# Add or remove model names as needed\n",
    "generation_models = [\n",
    "    'gemma3:1b',      # Example: Gemma 3 1B model\n",
    "    'gemma3:4b',      # Example: Gemma 3 4B model\n",
    "    'gemma3:12b',      # Example: Gemma 3 12B model\n",
    "    'gemma3:27b',     # Example: Gemma 3 16B model\n",
    "    'gpt-oss:20b',     # Example: GPT-OSS 20B model\n",
    "    'qwen3:latest',   # Example: Qwen 3 2B model\n",
    "    'gpt-oss:120b',    # Example: GPT-OSS 120B model\n",
    "    # Add more generation models here if you have them\n",
    "]\n",
    "\n",
    "# List of embedding models to test\n",
    "# These models will be used for creating embeddings\n",
    "# Add or remove model names as needed\n",
    "embedding_models = [\n",
    "    'embeddinggemma',  # Example: Gemma embedding model\n",
    "    'qwen3-embedding:8b-fp16',\n",
    "    'snowflake-arctic-embed2:568m',\n",
    "    'nomic-embed-text:137m-v1.5-fp16',\n",
    "    # Add more embedding models here if you have them\n",
    "]\n",
    "\n",
    "# Validate that model lists are not empty\n",
    "if len(generation_models) == 0:\n",
    "    raise ValueError(\"generation_models list cannot be empty! Please specify at least one generation model.\")\n",
    "if len(embedding_models) == 0:\n",
    "    raise ValueError(\"embedding_models list cannot be empty! Please specify at least one embedding model.\")\n",
    "\n",
    "# Store models for later use\n",
    "available_gen_models = generation_models\n",
    "available_embed_models = embedding_models\n",
    "\n",
    "print(\"âœ… Model configuration:\")\n",
    "print(f\"  Generation models ({len(available_gen_models)}): {available_gen_models}\")\n",
    "print(f\"  Embedding models ({len(available_embed_models)}): {available_embed_models}\")\n",
    "print(f\"  Total model combinations: {len(available_gen_models) * len(available_embed_models)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Parameter Ranges for NOLH Sampling\n",
    "# ============================================================================\n",
    "# These define the bounds of our experimental space\n",
    "# Now includes model indices as additional dimensions\n",
    "parameter_ranges = {\n",
    "    'num_docs': (5, 50),           # Number of document chunks: 5 to 50\n",
    "    'chunk_size': (256, 4000),      # Chunk size in tokens: 256 to 4000\n",
    "    'num_questions_per_doc': (1, 10),  # Questions per chunk: 1 to 10\n",
    "    'gen_model_idx': (0, len(available_gen_models) - 1),    # Generation model index (discrete)\n",
    "    'embed_model_idx': (0, len(available_embed_models) - 1)  # Embedding model index (discrete)\n",
    "}\n",
    "\n",
    "# Number of samples to generate\n",
    "# Recommended: 20-30 samples for 5 parameters (3 continuous + 2 discrete)\n",
    "# More samples = better coverage but more experiments to run\n",
    "num_samples = 30\n",
    "\n",
    "# Fixed parameters (same for all experiments)\n",
    "# Note: gen_model and embed_model are now sampled, not fixed\n",
    "fixed_config = {\n",
    "    'concurrency': 3,  # Sequential processing\n",
    "}\n",
    "\n",
    "# Pricing configurations (from notebook 04)\n",
    "# We'll calculate costs for BOTH API and local pricing for every experiment\n",
    "api_pricing_config = {\n",
    "    'mode': 'api',\n",
    "    'price_per_1k_tokens_input': 0.00250,\n",
    "    'price_per_1k_tokens_output': 0.01000\n",
    "}\n",
    "\n",
    "local_pricing_config = {\n",
    "    'mode': 'local',\n",
    "    'dollars_per_gpu_hour': 1.50\n",
    "}\n",
    "\n",
    "# Chunk overlap calculation: 10% of chunk_size, but capped at 50\n",
    "# This will be calculated per experiment based on chunk_size\n",
    "def calculate_chunk_overlap(chunk_size: int) -> int:\n",
    "    \"\"\"Calculate chunk overlap as 10% of chunk_size, capped at 50.\"\"\"\n",
    "    overlap = int(chunk_size * 0.1)\n",
    "    return min(overlap, 50)\n",
    "\n",
    "print(\"\\nâœ… Parameter configuration:\")\n",
    "print(f\"  Parameter ranges: {parameter_ranges}\")\n",
    "print(f\"  Number of samples: {num_samples}\")\n",
    "print(f\"  Fixed config: {fixed_config}\")\n",
    "print(f\"  API pricing: {api_pricing_config}\")\n",
    "print(f\"  Local pricing: {local_pricing_config}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Information Summary\n",
    "\n",
    "Display the model configuration and parameter ranges including model indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Model Configuration Summary\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Generation Models (7):\n",
      "   Index 0: gemma3:1b\n",
      "   Index 1: gemma3:4b\n",
      "   Index 2: gemma3:12b\n",
      "   Index 3: gemma3:27b\n",
      "   Index 4: gpt-oss:20b\n",
      "   Index 5: qwen3:latest\n",
      "   Index 6: gpt-oss:120b\n",
      "\n",
      "ðŸ“Š Embedding Models (4):\n",
      "   Index 0: embeddinggemma\n",
      "   Index 1: qwen3-embedding:8b-fp16\n",
      "   Index 2: snowflake-arctic-embed2:568m\n",
      "   Index 3: nomic-embed-text:137m-v1.5-fp16\n",
      "\n",
      "ðŸ“Š Total Model Combinations: 28\n",
      "   (Each NOLH sample will randomly select one generation model and one embedding model)\n",
      "\n",
      "ðŸ“Š Parameter Ranges:\n",
      "   num_docs: 5 to 50\n",
      "   chunk_size: 256 to 4000 tokens\n",
      "   num_questions_per_doc: 1 to 10\n",
      "   gen_model_idx: 0 to 6 (maps to generation models above)\n",
      "   embed_model_idx: 0 to 3 (maps to embedding models above)\n",
      "\n",
      "ðŸ“Š NOLH Design:\n",
      "   Dimensions: 5 (3 continuous + 2 discrete model indices)\n",
      "   Number of samples: 30\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display model information and parameter ranges\n",
    "print(\"=\"*70)\n",
    "print(\"Model Configuration Summary\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ“Š Generation Models ({len(available_gen_models)}):\")\n",
    "for idx, model in enumerate(available_gen_models):\n",
    "    print(f\"   Index {idx}: {model}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Embedding Models ({len(available_embed_models)}):\")\n",
    "for idx, model in enumerate(available_embed_models):\n",
    "    print(f\"   Index {idx}: {model}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Total Model Combinations: {len(available_gen_models) * len(available_embed_models)}\")\n",
    "print(f\"   (Each NOLH sample will randomly select one generation model and one embedding model)\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Parameter Ranges:\")\n",
    "print(f\"   num_docs: {parameter_ranges['num_docs'][0]} to {parameter_ranges['num_docs'][1]}\")\n",
    "print(f\"   chunk_size: {parameter_ranges['chunk_size'][0]} to {parameter_ranges['chunk_size'][1]} tokens\")\n",
    "print(f\"   num_questions_per_doc: {parameter_ranges['num_questions_per_doc'][0]} to {parameter_ranges['num_questions_per_doc'][1]}\")\n",
    "print(f\"   gen_model_idx: {parameter_ranges['gen_model_idx'][0]} to {parameter_ranges['gen_model_idx'][1]} (maps to generation models above)\")\n",
    "print(f\"   embed_model_idx: {parameter_ranges['embed_model_idx'][0]} to {parameter_ranges['embed_model_idx'][1]} (maps to embedding models above)\")\n",
    "\n",
    "print(f\"\\nðŸ“Š NOLH Design:\")\n",
    "print(f\"   Dimensions: 5 (3 continuous + 2 discrete model indices)\")\n",
    "print(f\"   Number of samples: {num_samples}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: NOLH Sample Generation\n",
    "\n",
    "Generate well-distributed sample points in the 5D parameter space using Latin Hypercube sampling.\n",
    "\n",
    "**Note**: The design now includes 5 dimensions:\n",
    "- 3 continuous: num_docs, chunk_size, num_questions_per_doc\n",
    "- 2 discrete: gen_model_idx, embed_model_idx (will be mapped to model names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 30 NOLH sample points (5 dimensions):\n",
      "\n",
      "Sample points (first 10):\n",
      " num_docs  chunk_size  num_questions_per_doc  gen_model_idx  embed_model_idx\n",
      "       43         326                      4              6                2\n",
      "       17        1783                      7              1                3\n",
      "       16        2137                      4              5                0\n",
      "       45        1560                      9              1                3\n",
      "       11        2708                      5              2                1\n",
      "       42        3567                      4              5                0\n",
      "       47        2756                      9              3                1\n",
      "       11        3485                      6              3                2\n",
      "       36         776                      8              4                1\n",
      "       20        1705                      6              4                2\n",
      "\n",
      "Parameter statistics:\n",
      "       num_docs   chunk_size  num_questions_per_doc  gen_model_idx  \\\n",
      "count  30.00000    30.000000              30.000000      30.000000   \n",
      "mean   27.50000  2123.766667               5.500000       3.000000   \n",
      "std    13.26065  1105.462196               2.596417       1.781127   \n",
      "min     6.00000   326.000000               1.000000       0.000000   \n",
      "25%    16.25000  1237.500000               3.250000       1.250000   \n",
      "50%    28.00000  2114.000000               5.500000       3.000000   \n",
      "75%    37.75000  3037.500000               7.750000       4.750000   \n",
      "max    50.00000  3959.000000              10.000000       6.000000   \n",
      "\n",
      "       embed_model_idx  \n",
      "count        30.000000  \n",
      "mean          1.500000  \n",
      "std           0.973795  \n",
      "min           0.000000  \n",
      "25%           1.000000  \n",
      "50%           1.500000  \n",
      "75%           2.000000  \n",
      "max           3.000000  \n",
      "\n",
      "Model index distributions:\n",
      "  gen_model_idx range: 0 to 6\n",
      "  embed_model_idx range: 0 to 3\n"
     ]
    }
   ],
   "source": [
    "# Generate Latin Hypercube samples\n",
    "# Latin Hypercube ensures good space-filling properties\n",
    "# We use scipy's qmc.LatinHypercube which is optimized for this purpose\n",
    "# Now generating 5-dimensional samples (3 continuous + 2 discrete model indices)\n",
    "sampler = qmc.LatinHypercube(d=5, seed=42)  # 5 dimensions: num_docs, chunk_size, num_questions, gen_model_idx, embed_model_idx\n",
    "samples = sampler.random(n=num_samples)  # Generate normalized samples [0, 1]\n",
    "\n",
    "# Map normalized samples [0, 1] to actual parameter ranges\n",
    "# This converts each dimension from [0, 1] to the actual parameter range\n",
    "nolh_samples = []\n",
    "for sample in samples:\n",
    "    # Map continuous dimensions (0, 1, 2)\n",
    "    # Dimension 0: num_docs\n",
    "    num_docs = int(np.round(\n",
    "        parameter_ranges['num_docs'][0] + \n",
    "        sample[0] * (parameter_ranges['num_docs'][1] - parameter_ranges['num_docs'][0])\n",
    "    ))\n",
    "    # Dimension 1: chunk_size\n",
    "    chunk_size = int(np.round(\n",
    "        parameter_ranges['chunk_size'][0] + \n",
    "        sample[1] * (parameter_ranges['chunk_size'][1] - parameter_ranges['chunk_size'][0])\n",
    "    ))\n",
    "    # Dimension 2: num_questions_per_doc\n",
    "    num_questions = int(np.round(\n",
    "        parameter_ranges['num_questions_per_doc'][0] + \n",
    "        sample[2] * (parameter_ranges['num_questions_per_doc'][1] - parameter_ranges['num_questions_per_doc'][0])\n",
    "    ))\n",
    "    \n",
    "    # Map discrete dimensions (3, 4) - model indices\n",
    "    # Dimension 3: gen_model_idx (discrete)\n",
    "    gen_model_idx_float = (\n",
    "        parameter_ranges['gen_model_idx'][0] + \n",
    "        sample[3] * (parameter_ranges['gen_model_idx'][1] - parameter_ranges['gen_model_idx'][0])\n",
    "    )\n",
    "    gen_model_idx = int(np.round(gen_model_idx_float))\n",
    "    \n",
    "    # Dimension 4: embed_model_idx (discrete)\n",
    "    embed_model_idx_float = (\n",
    "        parameter_ranges['embed_model_idx'][0] + \n",
    "        sample[4] * (parameter_ranges['embed_model_idx'][1] - parameter_ranges['embed_model_idx'][0])\n",
    "    )\n",
    "    embed_model_idx = int(np.round(embed_model_idx_float))\n",
    "    \n",
    "    # Ensure values are within bounds (rounding might push them slightly out)\n",
    "    num_docs = max(parameter_ranges['num_docs'][0], min(parameter_ranges['num_docs'][1], num_docs))\n",
    "    chunk_size = max(parameter_ranges['chunk_size'][0], min(parameter_ranges['chunk_size'][1], chunk_size))\n",
    "    num_questions = max(parameter_ranges['num_questions_per_doc'][0], \n",
    "                       min(parameter_ranges['num_questions_per_doc'][1], num_questions))\n",
    "    \n",
    "    # Clamp model indices to valid ranges\n",
    "    gen_model_idx = max(parameter_ranges['gen_model_idx'][0], \n",
    "                       min(parameter_ranges['gen_model_idx'][1], gen_model_idx))\n",
    "    embed_model_idx = max(parameter_ranges['embed_model_idx'][0], \n",
    "                         min(parameter_ranges['embed_model_idx'][1], embed_model_idx))\n",
    "    \n",
    "    nolh_samples.append({\n",
    "        'num_docs': num_docs,\n",
    "        'chunk_size': chunk_size,\n",
    "        'num_questions_per_doc': num_questions,\n",
    "        'gen_model_idx': gen_model_idx,      # Will be mapped to model name later\n",
    "        'embed_model_idx': embed_model_idx   # Will be mapped to model name later\n",
    "    })\n",
    "\n",
    "# Display the generated samples\n",
    "samples_df = pd.DataFrame(nolh_samples)\n",
    "print(f\"Generated {len(nolh_samples)} NOLH sample points (5 dimensions):\")\n",
    "print(\"\\nSample points (first 10):\")\n",
    "print(samples_df.head(10).to_string(index=False))\n",
    "print(f\"\\nParameter statistics:\")\n",
    "print(samples_df.describe())\n",
    "print(f\"\\nModel index distributions:\")\n",
    "print(f\"  gen_model_idx range: {samples_df['gen_model_idx'].min()} to {samples_df['gen_model_idx'].max()}\")\n",
    "print(f\"  embed_model_idx range: {samples_df['embed_model_idx'].min()} to {samples_df['embed_model_idx'].max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Experiment Configuration Generation\n",
    "\n",
    "Convert each NOLH sample point into a complete experiment configuration with proper naming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 30 experiment configurations\n",
      "\n",
      "First 5 experiments:\n",
      "  nolh_docs43_chunk326_q4_gengpt_oss_120b_embsnowflake_arctic_embed2_568m\n",
      "    docs=43, chunk=326, questions=4\n",
      "    gen_model=gpt-oss:120b, embed_model=snowflake-arctic-embed2:568m\n",
      "  nolh_docs17_chunk1783_q7_gengemma3_4b_embnomic_embed_text_137m_v1.5_fp16\n",
      "    docs=17, chunk=1783, questions=7\n",
      "    gen_model=gemma3:4b, embed_model=nomic-embed-text:137m-v1.5-fp16\n",
      "  nolh_docs16_chunk2137_q4_genqwen3_latest_embembeddinggemma\n",
      "    docs=16, chunk=2137, questions=4\n",
      "    gen_model=qwen3:latest, embed_model=embeddinggemma\n",
      "  nolh_docs45_chunk1560_q9_gengemma3_4b_embnomic_embed_text_137m_v1.5_fp16\n",
      "    docs=45, chunk=1560, questions=9\n",
      "    gen_model=gemma3:4b, embed_model=nomic-embed-text:137m-v1.5-fp16\n",
      "  nolh_docs11_chunk2708_q5_gengemma3_12b_embqwen3_embedding_8b_fp16\n",
      "    docs=11, chunk=2708, questions=5\n",
      "    gen_model=gemma3:12b, embed_model=qwen3-embedding:8b-fp16\n"
     ]
    }
   ],
   "source": [
    "# Convert NOLH samples to experiment configurations\n",
    "# Each experiment gets a name that includes its parameter values for easy identification\n",
    "# Model indices from NOLH samples are mapped to actual model names here\n",
    "experiments = []\n",
    "\n",
    "for i, sample in enumerate(nolh_samples):\n",
    "    # Map model indices to actual model names\n",
    "    # Extract model indices from the sample (these were generated by NOLH sampling)\n",
    "    gen_model_idx = sample['gen_model_idx']\n",
    "    embed_model_idx = sample['embed_model_idx']\n",
    "    \n",
    "    # Map integer indices to actual model names from the user-specified lists\n",
    "    # These lists were defined in Section 2\n",
    "    gen_model = available_gen_models[gen_model_idx]\n",
    "    embed_model = available_embed_models[embed_model_idx]\n",
    "    \n",
    "    # Generate experiment name with parameter labels including model info\n",
    "    # Format: nolh_docs{num}_chunk{size}_q{questions}_gen{model}_emb{model}\n",
    "    # Use shortened model names in the experiment name to keep it readable\n",
    "    gen_model_short = gen_model.replace(':', '_').replace('-', '_')\n",
    "    embed_model_short = embed_model.replace(':', '_').replace('-', '_')\n",
    "    exp_name = f\"nolh_docs{sample['num_docs']}_chunk{sample['chunk_size']}_q{sample['num_questions_per_doc']}_gen{gen_model_short}_emb{embed_model_short}\"\n",
    "    \n",
    "    # Calculate chunk overlap based on chunk_size\n",
    "    chunk_overlap = calculate_chunk_overlap(sample['chunk_size'])\n",
    "    \n",
    "    # Create experiment configuration dictionary\n",
    "    exp_config = {\n",
    "        'name': exp_name,\n",
    "        'gen_model': gen_model,              # Mapped from index to actual model name\n",
    "        'embed_model': embed_model,          # Mapped from index to actual model name\n",
    "        'num_docs': sample['num_docs'],\n",
    "        'chunk_size': sample['chunk_size'],\n",
    "        'chunk_overlap': chunk_overlap,\n",
    "        'num_questions_per_doc': sample['num_questions_per_doc'],\n",
    "        'concurrency': fixed_config['concurrency']\n",
    "    }\n",
    "    \n",
    "    experiments.append(exp_config)\n",
    "\n",
    "print(f\"Generated {len(experiments)} experiment configurations\")\n",
    "print(\"\\nFirst 5 experiments:\")\n",
    "for exp in experiments[:5]:\n",
    "    print(f\"  {exp['name']}\")\n",
    "    print(f\"    docs={exp['num_docs']}, chunk={exp['chunk_size']}, questions={exp['num_questions_per_doc']}\")\n",
    "    print(f\"    gen_model={exp['gen_model']}, embed_model={exp['embed_model']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Helper Functions\n",
    "\n",
    "Reuse helper functions from the cost sensitivity analysis notebook for cost calculation and experiment execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost calculation function defined\n"
     ]
    }
   ],
   "source": [
    "def calculate_costs(metrics: Dict[str, Any], api_pricing_config: Dict[str, Any], \n",
    "                    local_pricing_config: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calculate costs for BOTH API and local pricing models.\n",
    "    \n",
    "    This function takes aggregated metrics and both pricing configurations,\n",
    "    then computes costs for both API pricing (per-token) and local pricing (per-hour).\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary with aggregated metrics (from aggregate_metrics)\n",
    "        api_pricing_config: API pricing configuration dict\n",
    "        local_pricing_config: Local pricing configuration dict\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "        - 'api_costs': Dictionary with API cost breakdown\n",
    "        - 'local_costs': Dictionary with local cost breakdown\n",
    "        - 'total_input_tokens', 'total_output_tokens', 'total_tokens', 'total_runtime_seconds'\n",
    "    \"\"\"\n",
    "    # Extract token and time metrics from the aggregated metrics dictionary\n",
    "    # These come from the embedding and inference operations\n",
    "    embedding_tokens = metrics.get('embedding', {}).get('total_tokens', 0)\n",
    "    inference_prompt_tokens = metrics.get('inference', {}).get('total_prompt_tokens', 0)\n",
    "    inference_response_tokens = metrics.get('inference', {}).get('total_response_tokens', 0)\n",
    "    \n",
    "    # Calculate total input tokens (embedding + prompt tokens) and output tokens (response tokens)\n",
    "    total_input_tokens = embedding_tokens + inference_prompt_tokens\n",
    "    total_output_tokens = inference_response_tokens\n",
    "    total_tokens = total_input_tokens + total_output_tokens\n",
    "    \n",
    "    # Get total runtime in seconds\n",
    "    total_runtime_seconds = metrics.get('overall', {}).get('total_time_seconds', 0)\n",
    "    \n",
    "    # Calculate API costs (per-token pricing)\n",
    "    price_per_1k_input = api_pricing_config.get('price_per_1k_tokens_input', 0.0)\n",
    "    price_per_1k_output = api_pricing_config.get('price_per_1k_tokens_output', 0.0)\n",
    "    \n",
    "    api_cost_input = (total_input_tokens / 1000.0) * price_per_1k_input\n",
    "    api_cost_output = (total_output_tokens / 1000.0) * price_per_1k_output\n",
    "    api_total_cost = api_cost_input + api_cost_output\n",
    "    \n",
    "    api_costs = {\n",
    "        'cost_input': api_cost_input,\n",
    "        'cost_output': api_cost_output,\n",
    "        'total_cost': api_total_cost,\n",
    "        'cost_per_1k_tokens': (api_total_cost / total_tokens * 1000) if total_tokens > 0 else 0,\n",
    "        'pricing_mode': 'api'\n",
    "    }\n",
    "    \n",
    "    # Calculate local costs (per-hour pricing)\n",
    "    dollars_per_hour = local_pricing_config.get('dollars_per_gpu_hour', 0.0)\n",
    "    runtime_hours = total_runtime_seconds / 3600.0\n",
    "    local_total_cost = runtime_hours * dollars_per_hour\n",
    "    \n",
    "    local_costs = {\n",
    "        'runtime_hours': runtime_hours,\n",
    "        'total_cost': local_total_cost,\n",
    "        'cost_per_1k_tokens': (local_total_cost / total_tokens * 1000) if total_tokens > 0 else 0,\n",
    "        'pricing_mode': 'local'\n",
    "    }\n",
    "    \n",
    "    # Return combined results\n",
    "    return {\n",
    "        'api_costs': api_costs,\n",
    "        'local_costs': local_costs,\n",
    "        'total_input_tokens': total_input_tokens,\n",
    "        'total_output_tokens': total_output_tokens,\n",
    "        'total_tokens': total_tokens,\n",
    "        'total_runtime_seconds': total_runtime_seconds\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Cost calculation function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment runner function defined\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(exp_config: Dict[str, Any], document_path: Path, \n",
    "                  experiments_dir: Path, reuse_chunks: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run a single experiment.\n",
    "    \n",
    "    This function orchestrates the entire experiment pipeline:\n",
    "    1. Creates a Config object from the experiment configuration\n",
    "    2. Loads and chunks the document (or reuses existing chunks)\n",
    "    3. Runs embedding pipeline for the specified number of chunks\n",
    "    4. Runs question generation pipeline\n",
    "    5. Collects and aggregates all metrics\n",
    "    6. Calculates costs\n",
    "    7. Returns experiment results\n",
    "    \n",
    "    Args:\n",
    "        exp_config: Experiment configuration dictionary with all parameters\n",
    "        document_path: Path to the document file to process\n",
    "        experiments_dir: Directory to save experiment results\n",
    "        reuse_chunks: If True, try to reuse existing chunks from previous runs\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with experiment results including metrics and costs\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Running experiment: {exp_config['name']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create Config object from experiment configuration\n",
    "    # This config object will be used throughout the pipeline\n",
    "    config = Config(\n",
    "        embedding_model=exp_config['embed_model'],\n",
    "        generation_model=exp_config['gen_model'],\n",
    "        chunk_size_tokens=exp_config['chunk_size'],\n",
    "        chunk_overlap_tokens=exp_config.get('chunk_overlap', 50),\n",
    "        chunking_strategy='fixed_token_window'\n",
    "    )\n",
    "    \n",
    "    # Create metrics store for this experiment\n",
    "    # This will track all token counts and timings\n",
    "    metrics_store = MetricsStore()\n",
    "    \n",
    "    # Load and chunk document (or reuse existing chunks)\n",
    "    # For efficiency, we can reuse chunks if they exist and match our config\n",
    "    chunks_path = config.get_chunks_path()\n",
    "    \n",
    "    if reuse_chunks and chunks_path.exists():\n",
    "        # Try to load existing chunks from a previous run\n",
    "        try:\n",
    "            with open(chunks_path, 'r') as f:\n",
    "                all_chunks = json.load(f)\n",
    "            print(f\"Loaded {len(all_chunks)} existing chunks\")\n",
    "        except:\n",
    "            all_chunks = None\n",
    "    else:\n",
    "        all_chunks = None\n",
    "    \n",
    "    if all_chunks is None:\n",
    "        # Need to create chunks from scratch\n",
    "        print(\"Loading and chunking document...\")\n",
    "        text = load_document(document_path)\n",
    "        all_chunks = chunk_text(text, config)\n",
    "        print(f\"Created {len(all_chunks)} chunks\")\n",
    "    \n",
    "    # Select subset of chunks based on num_docs\n",
    "    # This allows us to test with different numbers of documents\n",
    "    num_docs = exp_config.get('num_docs')\n",
    "    if num_docs is None or num_docs > len(all_chunks):\n",
    "        selected_chunks = all_chunks\n",
    "    else:\n",
    "        selected_chunks = all_chunks[:num_docs]\n",
    "    \n",
    "    print(f\"Processing {len(selected_chunks)} chunks for this experiment\")\n",
    "    \n",
    "    # Extract chunk text (remove embeddings if present to save memory)\n",
    "    # We'll regenerate embeddings anyway, so we don't need old ones\n",
    "    chunks_for_embedding = []\n",
    "    for chunk in selected_chunks:\n",
    "        chunk_copy = {k: v for k, v in chunk.items() if k != 'embedding'}\n",
    "        chunks_for_embedding.append(chunk_copy)\n",
    "    \n",
    "    # Run embedding pipeline\n",
    "    # This generates vector embeddings for each chunk\n",
    "    print(\"Embedding chunks...\")\n",
    "    embedded_chunks = embed_chunks(chunks_for_embedding, config, metrics_store)\n",
    "    print(f\"Embedded {len(embedded_chunks)} chunks\")\n",
    "    \n",
    "    # Run question generation pipeline\n",
    "    # This generates questions for each chunk using the LLM\n",
    "    print(f\"Generating questions ({exp_config['num_questions_per_doc']} per chunk)...\")\n",
    "    all_questions = []\n",
    "    \n",
    "    # Process chunks sequentially (concurrency can be added later if needed)\n",
    "    for chunk in embedded_chunks:\n",
    "        questions = generate_questions(\n",
    "            chunk,\n",
    "            exp_config['num_questions_per_doc'],\n",
    "            config,\n",
    "            metrics_store\n",
    "        )\n",
    "        all_questions.extend(questions)\n",
    "    \n",
    "    print(f\"Generated {len(all_questions)} questions\")\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    # Convert metrics store to DataFrame for aggregation\n",
    "    metrics_df = pd.DataFrame(metrics_store.metrics)\n",
    "    aggregated = aggregate_metrics(metrics_df)\n",
    "    \n",
    "    # Calculate costs for BOTH API and local pricing models\n",
    "    # Note: api_pricing_config and local_pricing_config are defined in Section 2\n",
    "    costs = calculate_costs(aggregated, api_pricing_config, local_pricing_config)\n",
    "    \n",
    "    # Extract cost values for easier access\n",
    "    api_costs = costs['api_costs']\n",
    "    local_costs = costs['local_costs']\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    total_questions = len(all_questions)\n",
    "    total_chunks = len(embedded_chunks)\n",
    "    \n",
    "    # Calculate derived metrics like tokens per second\n",
    "    tokens_per_second = aggregated.get('overall', {}).get('tokens_per_second', 0)\n",
    "    \n",
    "    # Calculate cost difference and ratio\n",
    "    cost_difference = api_costs['total_cost'] - local_costs['total_cost']\n",
    "    cost_ratio = api_costs['total_cost'] / local_costs['total_cost'] if local_costs['total_cost'] > 0 else 0\n",
    "    \n",
    "    # Build experiment results dictionary\n",
    "    # This contains all the information we'll analyze later\n",
    "    results = {\n",
    "        'experiment_name': exp_config['name'],\n",
    "        'gen_model': exp_config['gen_model'],\n",
    "        'embed_model': exp_config['embed_model'],\n",
    "        'num_docs': len(selected_chunks),\n",
    "        'chunk_size': exp_config['chunk_size'],\n",
    "        'chunk_overlap': exp_config.get('chunk_overlap', 50),\n",
    "        'num_questions_per_doc': exp_config['num_questions_per_doc'],\n",
    "        'total_questions': total_questions,\n",
    "        'concurrency': exp_config.get('concurrency', 1),\n",
    "        'total_input_tokens': costs['total_input_tokens'],\n",
    "        'total_output_tokens': costs['total_output_tokens'],\n",
    "        'total_tokens': costs['total_tokens'],\n",
    "        'total_runtime_seconds': costs['total_runtime_seconds'],\n",
    "        'tokens_per_second': tokens_per_second,\n",
    "        # API costs\n",
    "        'cost_api_total': api_costs['total_cost'],\n",
    "        'cost_api_per_1k_tokens': api_costs['cost_per_1k_tokens'],\n",
    "        'cost_api_per_document': api_costs['total_cost'] / len(selected_chunks) if len(selected_chunks) > 0 else 0,\n",
    "        'cost_api_per_question': api_costs['total_cost'] / total_questions if total_questions > 0 else 0,\n",
    "        'cost_api_input': api_costs['cost_input'],\n",
    "        'cost_api_output': api_costs['cost_output'],\n",
    "        # Local costs\n",
    "        'cost_local_total': local_costs['total_cost'],\n",
    "        'cost_local_per_1k_tokens': local_costs['cost_per_1k_tokens'],\n",
    "        'cost_local_per_document': local_costs['total_cost'] / len(selected_chunks) if len(selected_chunks) > 0 else 0,\n",
    "        'cost_local_per_question': local_costs['total_cost'] / total_questions if total_questions > 0 else 0,\n",
    "        'cost_local_runtime_hours': local_costs['runtime_hours'],\n",
    "        # Cost comparison metrics\n",
    "        'cost_difference': cost_difference,  # API - Local (positive = API more expensive)\n",
    "        'cost_ratio': cost_ratio,  # API / Local (ratio > 1 = API more expensive)\n",
    "        # Backward compatibility (use API costs as default)\n",
    "        'total_cost': api_costs['total_cost'],\n",
    "        'cost_per_1k_tokens': api_costs['cost_per_1k_tokens'],\n",
    "        'cost_per_document': api_costs['total_cost'] / len(selected_chunks) if len(selected_chunks) > 0 else 0,\n",
    "        'cost_per_question': api_costs['total_cost'] / total_questions if total_questions > 0 else 0,\n",
    "        'pricing_mode': 'both',  # Indicates both pricing models were calculated\n",
    "        'aggregated_metrics': aggregated,\n",
    "        'costs': costs\n",
    "    }\n",
    "    \n",
    "    # Save experiment metrics to disk\n",
    "    exp_metrics_path = experiments_dir / f\"{exp_config['name']}_metrics.json\"\n",
    "    save_metrics(metrics_store, exp_metrics_path)\n",
    "    print(f\"Saved metrics to {exp_metrics_path}\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Experiment completed in {elapsed:.2f} seconds\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Experiment runner function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Experiment Execution Loop\n",
    "\n",
    "Run all NOLH-generated experiments and collect results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting execution of 30 experiments...\n",
      "Total estimated time: ~60 minutes (rough estimate)\n",
      "\n",
      "[1/30] Processing experiment: nolh_docs43_chunk326_q4_gengpt_oss_120b_embsnowflake_arctic_embed2_568m\n",
      "\n",
      "======================================================================\n",
      "Running experiment: nolh_docs43_chunk326_q4_gengpt_oss_120b_embsnowflake_arctic_embed2_568m\n",
      "======================================================================\n",
      "Loaded 772 existing chunks\n",
      "Processing 43 chunks for this experiment\n",
      "Embedding chunks...\n",
      "Embedded 43 chunks\n",
      "Generating questions (4 per chunk)...\n"
     ]
    }
   ],
   "source": [
    "# Execute all experiments\n",
    "# This loop runs each experiment configuration and collects the results\n",
    "all_results = []\n",
    "failed_experiments = []\n",
    "\n",
    "print(f\"Starting execution of {len(experiments)} experiments...\")\n",
    "print(f\"Total estimated time: ~{len(experiments) * 2} minutes (rough estimate)\")\n",
    "\n",
    "for i, exp_config in enumerate(experiments, 1):\n",
    "    try:\n",
    "        print(f\"\\n[{i}/{len(experiments)}] Processing experiment: {exp_config['name']}\")\n",
    "        \n",
    "        # Run the experiment\n",
    "        result = run_experiment(\n",
    "            exp_config, \n",
    "            document_path, \n",
    "            experiments_dir,\n",
    "            reuse_chunks=True  # Reuse chunks to save time\n",
    "        )\n",
    "        \n",
    "        # Add NOLH sample point coordinates to results for analysis\n",
    "        # Find the corresponding sample point\n",
    "        sample_idx = i - 1\n",
    "        result['nolh_sample_idx'] = sample_idx\n",
    "        result['nolh_num_docs'] = nolh_samples[sample_idx]['num_docs']\n",
    "        result['nolh_chunk_size'] = nolh_samples[sample_idx]['chunk_size']\n",
    "        result['nolh_num_questions'] = nolh_samples[sample_idx]['num_questions_per_doc']\n",
    "        \n",
    "        all_results.append(result)\n",
    "        print(f\"âœ“ Experiment {i} completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Experiment {i} failed: {str(e)}\")\n",
    "        failed_experiments.append({\n",
    "            'experiment': exp_config['name'],\n",
    "            'error': str(e)\n",
    "        })\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Experiment execution complete!\")\n",
    "print(f\"  Successful: {len(all_results)}\")\n",
    "print(f\"  Failed: {len(failed_experiments)}\")\n",
    "if failed_experiments:\n",
    "    print(f\"\\nFailed experiments:\")\n",
    "    for fail in failed_experiments:\n",
    "        print(f\"  - {fail['experiment']}: {fail['error']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Results Aggregation\n",
    "\n",
    "Collect all experiment results into a summary DataFrame for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build summary DataFrame from all results\n",
    "# Extract key metrics from each experiment result\n",
    "summary_data = []\n",
    "\n",
    "for result in all_results:\n",
    "    summary_data.append({\n",
    "        'experiment_name': result['experiment_name'],\n",
    "        'gen_model': result['gen_model'],\n",
    "        'embed_model': result['embed_model'],\n",
    "        'num_docs': result['num_docs'],\n",
    "        'chunk_size': result['chunk_size'],\n",
    "        'chunk_overlap': result['chunk_overlap'],\n",
    "        'num_questions_per_doc': result['num_questions_per_doc'],\n",
    "        'total_questions': result['total_questions'],\n",
    "        'concurrency': result['concurrency'],\n",
    "        'total_input_tokens': result['total_input_tokens'],\n",
    "        'total_output_tokens': result['total_output_tokens'],\n",
    "        'total_tokens': result['total_tokens'],\n",
    "        'total_runtime_seconds': result['total_runtime_seconds'],\n",
    "        'tokens_per_second': result['tokens_per_second'],\n",
    "        # API costs\n",
    "        'cost_api_total': result.get('cost_api_total', 0),\n",
    "        'cost_api_per_1k_tokens': result.get('cost_api_per_1k_tokens', 0),\n",
    "        'cost_api_per_document': result.get('cost_api_per_document', 0),\n",
    "        'cost_api_per_question': result.get('cost_api_per_question', 0),\n",
    "        'cost_api_input': result.get('cost_api_input', 0),\n",
    "        'cost_api_output': result.get('cost_api_output', 0),\n",
    "        # Local costs\n",
    "        'cost_local_total': result.get('cost_local_total', 0),\n",
    "        'cost_local_per_1k_tokens': result.get('cost_local_per_1k_tokens', 0),\n",
    "        'cost_local_per_document': result.get('cost_local_per_document', 0),\n",
    "        'cost_local_per_question': result.get('cost_local_per_question', 0),\n",
    "        'cost_local_runtime_hours': result.get('cost_local_runtime_hours', 0),\n",
    "        # Cost comparison metrics\n",
    "        'cost_difference': result.get('cost_difference', 0),  # API - Local\n",
    "        'cost_ratio': result.get('cost_ratio', 0),  # API / Local\n",
    "        # Backward compatibility fields\n",
    "        'total_cost': result.get('total_cost', result.get('cost_api_total', 0)),\n",
    "        'cost_per_1k_tokens': result.get('cost_per_1k_tokens', result.get('cost_api_per_1k_tokens', 0)),\n",
    "        'cost_per_document': result.get('cost_per_document', result.get('cost_api_per_document', 0)),\n",
    "        'cost_per_question': result.get('cost_per_question', result.get('cost_api_per_question', 0)),\n",
    "        'pricing_mode': result.get('pricing_mode', 'both'),\n",
    "        # Include NOLH sample coordinates\n",
    "        'nolh_sample_idx': result.get('nolh_sample_idx', -1),\n",
    "        'nolh_num_docs': result.get('nolh_num_docs', result['num_docs']),\n",
    "        'nolh_chunk_size': result.get('nolh_chunk_size', result['chunk_size']),\n",
    "        'nolh_num_questions': result.get('nolh_num_questions', result['num_questions_per_doc'])\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save summary to CSV\n",
    "summary_path = experiments_dir / \"nolh_summary.csv\"\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"Summary saved to: {summary_path}\")\n",
    "print(f\"\\nSummary statistics:\")\n",
    "print(summary_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display formatted summary table\n",
    "# Show key metrics in a readable format with both API and local costs\n",
    "display_columns = [\n",
    "    'experiment_name', 'num_docs', 'chunk_size', 'num_questions_per_doc',\n",
    "    'total_tokens', 'total_runtime_seconds',\n",
    "    'cost_api_total', 'cost_local_total', 'cost_difference', 'cost_ratio',\n",
    "    'cost_api_per_1k_tokens', 'cost_local_per_1k_tokens',\n",
    "    'cost_api_per_document', 'cost_local_per_document'\n",
    "]\n",
    "\n",
    "print(\"Experiment Summary (API vs Local Costs):\")\n",
    "print(\"=\" * 150)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 30)\n",
    "\n",
    "# Format numeric columns for better readability\n",
    "display_df = summary_df[display_columns].copy()\n",
    "display_df['cost_api_total'] = display_df['cost_api_total'].apply(lambda x: f\"${x:.4f}\")\n",
    "display_df['cost_local_total'] = display_df['cost_local_total'].apply(lambda x: f\"${x:.4f}\")\n",
    "display_df['cost_difference'] = display_df['cost_difference'].apply(lambda x: f\"${x:+.4f}\")  # + sign shows positive/negative\n",
    "display_df['cost_ratio'] = display_df['cost_ratio'].apply(lambda x: f\"{x:.3f}x\")\n",
    "display_df['cost_api_per_1k_tokens'] = display_df['cost_api_per_1k_tokens'].apply(lambda x: f\"${x:.4f}\")\n",
    "display_df['cost_local_per_1k_tokens'] = display_df['cost_local_per_1k_tokens'].apply(lambda x: f\"${x:.4f}\")\n",
    "display_df['cost_api_per_document'] = display_df['cost_api_per_document'].apply(lambda x: f\"${x:.4f}\")\n",
    "display_df['cost_local_per_document'] = display_df['cost_local_per_document'].apply(lambda x: f\"${x:.4f}\")\n",
    "display_df['total_runtime_seconds'] = display_df['total_runtime_seconds'].apply(lambda x: f\"{x:.2f}s\")\n",
    "\n",
    "try:\n",
    "    display(display_df)\n",
    "except:\n",
    "    print(display_df.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Analysis and Visualization\n",
    "\n",
    "Create visualizations to understand parameter relationships and their impact on cost, tokens, and runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 3D Scatter Plot: Parameter Space with Cost Coloring\n",
    "# This shows how cost varies across the 3D parameter space\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Create 3D subplot\n",
    "ax1 = fig.add_subplot(221, projection='3d')\n",
    "scatter = ax1.scatter(\n",
    "    summary_df['num_docs'],\n",
    "    summary_df['chunk_size'],\n",
    "    summary_df['num_questions_per_doc'],\n",
    "    c=summary_df['total_cost'],\n",
    "    cmap='viridis',\n",
    "    s=100,\n",
    "    alpha=0.7\n",
    ")\n",
    "ax1.set_xlabel('Number of Docs')\n",
    "ax1.set_ylabel('Chunk Size (tokens)')\n",
    "ax1.set_zlabel('Questions per Doc')\n",
    "ax1.set_title('3D Parameter Space\\n(Color = Total Cost)')\n",
    "plt.colorbar(scatter, ax=ax1, label='Total Cost ($)')\n",
    "\n",
    "# 2. Pairwise Parameter Plots\n",
    "# These show relationships between pairs of parameters\n",
    "\n",
    "# num_docs vs chunk_size\n",
    "ax2 = fig.add_subplot(222)\n",
    "scatter2 = ax2.scatter(\n",
    "    summary_df['num_docs'],\n",
    "    summary_df['chunk_size'],\n",
    "    c=summary_df['total_cost'],\n",
    "    cmap='viridis',\n",
    "    s=100,\n",
    "    alpha=0.7\n",
    ")\n",
    "ax2.set_xlabel('Number of Docs')\n",
    "ax2.set_ylabel('Chunk Size (tokens)')\n",
    "ax2.set_title('Docs vs Chunk Size\\n(Color = Total Cost)')\n",
    "plt.colorbar(scatter2, ax=ax2, label='Total Cost ($)')\n",
    "\n",
    "# num_docs vs num_questions_per_doc\n",
    "ax3 = fig.add_subplot(223)\n",
    "scatter3 = ax3.scatter(\n",
    "    summary_df['num_docs'],\n",
    "    summary_df['num_questions_per_doc'],\n",
    "    c=summary_df['total_cost'],\n",
    "    cmap='viridis',\n",
    "    s=100,\n",
    "    alpha=0.7\n",
    ")\n",
    "ax3.set_xlabel('Number of Docs')\n",
    "ax3.set_ylabel('Questions per Doc')\n",
    "ax3.set_title('Docs vs Questions\\n(Color = Total Cost)')\n",
    "plt.colorbar(scatter3, ax=ax3, label='Total Cost ($)')\n",
    "\n",
    "# chunk_size vs num_questions_per_doc\n",
    "ax4 = fig.add_subplot(224)\n",
    "scatter4 = ax4.scatter(\n",
    "    summary_df['chunk_size'],\n",
    "    summary_df['num_questions_per_doc'],\n",
    "    c=summary_df['total_cost'],\n",
    "    cmap='viridis',\n",
    "    s=100,\n",
    "    alpha=0.7\n",
    ")\n",
    "ax4.set_xlabel('Chunk Size (tokens)')\n",
    "ax4.set_ylabel('Questions per Doc')\n",
    "ax4.set_title('Chunk Size vs Questions\\n(Color = Total Cost)')\n",
    "plt.colorbar(scatter4, ax=ax4, label='Total Cost ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(experiments_dir / 'nolh_parameter_space_analysis.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved parameter space analysis plot to: {experiments_dir / 'nolh_parameter_space_analysis.png'}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response Surface Plots: Cost vs Parameters\n",
    "# These show how cost changes as we vary each parameter\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Cost vs Number of Docs\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(summary_df['num_docs'], summary_df['total_cost'], alpha=0.6, s=80)\n",
    "# Add trend line\n",
    "z = np.polyfit(summary_df['num_docs'], summary_df['total_cost'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax1.plot(summary_df['num_docs'].sort_values(), p(summary_df['num_docs'].sort_values()), \n",
    "         \"r--\", alpha=0.8, label=f'Trend: y={z[0]:.4f}x+{z[1]:.4f}')\n",
    "ax1.set_xlabel('Number of Docs')\n",
    "ax1.set_ylabel('Total Cost ($)')\n",
    "ax1.set_title('Cost vs Number of Docs')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Cost vs Chunk Size\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(summary_df['chunk_size'], summary_df['total_cost'], alpha=0.6, s=80)\n",
    "z = np.polyfit(summary_df['chunk_size'], summary_df['total_cost'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax2.plot(summary_df['chunk_size'].sort_values(), p(summary_df['chunk_size'].sort_values()), \n",
    "         \"r--\", alpha=0.8, label=f'Trend: y={z[0]:.6f}x+{z[1]:.4f}')\n",
    "ax2.set_xlabel('Chunk Size (tokens)')\n",
    "ax2.set_ylabel('Total Cost ($)')\n",
    "ax2.set_title('Cost vs Chunk Size')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Cost vs Questions per Doc\n",
    "ax3 = axes[1, 0]\n",
    "ax3.scatter(summary_df['num_questions_per_doc'], summary_df['total_cost'], alpha=0.6, s=80)\n",
    "z = np.polyfit(summary_df['num_questions_per_doc'], summary_df['total_cost'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax3.plot(summary_df['num_questions_per_doc'].sort_values(), \n",
    "         p(summary_df['num_questions_per_doc'].sort_values()), \n",
    "         \"r--\", alpha=0.8, label=f'Trend: y={z[0]:.4f}x+{z[1]:.4f}')\n",
    "ax3.set_xlabel('Questions per Doc')\n",
    "ax3.set_ylabel('Total Cost ($)')\n",
    "ax3.set_title('Cost vs Questions per Doc')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Runtime vs Total Tokens\n",
    "ax4 = axes[1, 1]\n",
    "ax4.scatter(summary_df['total_tokens'], summary_df['total_runtime_seconds'], alpha=0.6, s=80)\n",
    "z = np.polyfit(summary_df['total_tokens'], summary_df['total_runtime_seconds'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax4.plot(summary_df['total_tokens'].sort_values(), \n",
    "         p(summary_df['total_tokens'].sort_values()), \n",
    "         \"r--\", alpha=0.8, label=f'Trend: y={z[0]:.6f}x+{z[1]:.2f}')\n",
    "ax4.set_xlabel('Total Tokens')\n",
    "ax4.set_ylabel('Runtime (seconds)')\n",
    "ax4.set_title('Runtime vs Total Tokens')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(experiments_dir / 'nolh_response_surfaces.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved response surface plots to: {experiments_dir / 'nolh_response_surfaces.png'}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8.1: API vs Local Cost Comparison\n",
    "\n",
    "Visualizations that clearly show the differences between API and local pricing models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Side-by-Side Cost Comparison\n",
    "# Bar chart showing API cost vs Local cost for each experiment\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Select a subset of experiments for readability (or show all if not too many)\n",
    "num_to_show = min(30, len(summary_df))\n",
    "display_df = summary_df.head(num_to_show).copy()\n",
    "\n",
    "x = np.arange(len(display_df))\n",
    "width = 0.35\n",
    "\n",
    "# Create grouped bars\n",
    "bars1 = ax.bar(x - width/2, display_df['cost_api_total'], width, label='API Cost', alpha=0.8, color='#2E86AB')\n",
    "bars2 = ax.bar(x + width/2, display_df['cost_local_total'], width, label='Local Cost', alpha=0.8, color='#A23B72')\n",
    "\n",
    "ax.set_xlabel('Experiment Index', fontsize=12)\n",
    "ax.set_ylabel('Total Cost ($)', fontsize=12)\n",
    "ax.set_title(f'Side-by-Side Cost Comparison: API vs Local (First {num_to_show} Experiments)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"E{i+1}\" for i in range(len(display_df))], rotation=45, ha='right')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if height > 0.01:  # Only label if significant\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'${height:.3f}',\n",
    "                   ha='center', va='bottom', fontsize=7, rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(experiments_dir / 'nolh_cost_comparison_bars.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved side-by-side cost comparison to: {experiments_dir / 'nolh_cost_comparison_bars.png'}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Scatter Plot: API vs Local Costs\n",
    "# X-axis: Local cost, Y-axis: API cost\n",
    "# Points above diagonal = API more expensive, below = Local more expensive\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Color points by number of docs to show parameter relationships\n",
    "scatter = ax.scatter(\n",
    "    summary_df['cost_local_total'],\n",
    "    summary_df['cost_api_total'],\n",
    "    c=summary_df['num_docs'],\n",
    "    cmap='viridis',\n",
    "    s=100,\n",
    "    alpha=0.7,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "# Add diagonal reference line (y=x) where costs are equal\n",
    "max_cost = max(summary_df['cost_api_total'].max(), summary_df['cost_local_total'].max())\n",
    "ax.plot([0, max_cost], [0, max_cost], 'r--', linewidth=2, label='Equal Cost Line', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Local Cost ($)', fontsize=12)\n",
    "ax.set_ylabel('API Cost ($)', fontsize=12)\n",
    "ax.set_title('API Cost vs Local Cost\\n(Points above line = API more expensive)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Number of Docs', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(experiments_dir / 'nolh_cost_scatter_comparison.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved API vs Local cost scatter plot to: {experiments_dir / 'nolh_cost_scatter_comparison.png'}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Cost Difference Analysis\n",
    "# Histogram of cost differences (API - Local)\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create histogram\n",
    "n, bins, patches = ax.hist(summary_df['cost_difference'], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "\n",
    "# Color bars: red for positive (API more expensive), green for negative (Local more expensive)\n",
    "for i, (bar, bin_val) in enumerate(zip(patches, bins[:-1])):\n",
    "    if bin_val >= 0:\n",
    "        bar.set_color('coral')  # API more expensive\n",
    "    else:\n",
    "        bar.set_color('lightgreen')  # Local more expensive\n",
    "\n",
    "# Add vertical line at 0 to show break-even point\n",
    "ax.axvline(x=0, color='black', linestyle='--', linewidth=2, label='Break-even (API = Local)')\n",
    "\n",
    "# Add statistics text\n",
    "mean_diff = summary_df['cost_difference'].mean()\n",
    "median_diff = summary_df['cost_difference'].median()\n",
    "ax.axvline(x=mean_diff, color='red', linestyle=':', linewidth=2, label=f'Mean: ${mean_diff:.4f}')\n",
    "ax.axvline(x=median_diff, color='blue', linestyle=':', linewidth=2, label=f'Median: ${median_diff:.4f}')\n",
    "\n",
    "ax.set_xlabel('Cost Difference: API - Local ($)', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('Distribution of Cost Differences\\n(Positive = API more expensive, Negative = Local more expensive)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add text box with summary statistics\n",
    "api_cheaper = (summary_df['cost_difference'] < 0).sum()\n",
    "local_cheaper = (summary_df['cost_difference'] > 0).sum()\n",
    "equal = (summary_df['cost_difference'] == 0).sum()\n",
    "total = len(summary_df)\n",
    "\n",
    "textstr = f'API Cheaper: {api_cheaper} ({api_cheaper/total*100:.1f}%)\\n'\n",
    "textstr += f'Local Cheaper: {local_cheaper} ({local_cheaper/total*100:.1f}%)\\n'\n",
    "textstr += f'Equal: {equal} ({equal/total*100:.1f}%)'\n",
    "\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(experiments_dir / 'nolh_cost_difference_histogram.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved cost difference histogram to: {experiments_dir / 'nolh_cost_difference_histogram.png'}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) Cost Ratio Analysis\n",
    "# Scatter plot of cost ratio (API/Local) vs total tokens\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Cost ratio vs Total Tokens\n",
    "ax1 = axes[0]\n",
    "scatter1 = ax1.scatter(\n",
    "    summary_df['total_tokens'],\n",
    "    summary_df['cost_ratio'],\n",
    "    c=summary_df['num_docs'],\n",
    "    cmap='coolwarm',\n",
    "    s=100,\n",
    "    alpha=0.7,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "ax1.axhline(y=1.0, color='black', linestyle='--', linewidth=2, label='Ratio = 1.0 (Equal Cost)')\n",
    "ax1.set_xlabel('Total Tokens', fontsize=12)\n",
    "ax1.set_ylabel('Cost Ratio (API / Local)', fontsize=12)\n",
    "ax1.set_title('Cost Ratio vs Total Tokens\\n(Ratio > 1 = API more expensive)', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "cbar1 = plt.colorbar(scatter1, ax=ax1)\n",
    "cbar1.set_label('Number of Docs', fontsize=11)\n",
    "\n",
    "# Plot 2: Cost ratio vs Runtime\n",
    "ax2 = axes[1]\n",
    "scatter2 = ax2.scatter(\n",
    "    summary_df['total_runtime_seconds'],\n",
    "    summary_df['cost_ratio'],\n",
    "    c=summary_df['chunk_size'],\n",
    "    cmap='plasma',\n",
    "    s=100,\n",
    "    alpha=0.7,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "ax2.axhline(y=1.0, color='black', linestyle='--', linewidth=2, label='Ratio = 1.0 (Equal Cost)')\n",
    "ax2.set_xlabel('Runtime (seconds)', fontsize=12)\n",
    "ax2.set_ylabel('Cost Ratio (API / Local)', fontsize=12)\n",
    "ax2.set_title('Cost Ratio vs Runtime\\n(Ratio > 1 = API more expensive)', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "cbar2 = plt.colorbar(scatter2, ax=ax2)\n",
    "cbar2.set_label('Chunk Size', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(experiments_dir / 'nolh_cost_ratio_analysis.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved cost ratio analysis to: {experiments_dir / 'nolh_cost_ratio_analysis.png'}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) Parameter Space with Cost Difference Coloring\n",
    "# 3D scatter plot colored by cost difference (diverging colormap)\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Create 3D subplot\n",
    "ax1 = fig.add_subplot(221, projection='3d')\n",
    "# Use diverging colormap centered at 0\n",
    "scatter = ax1.scatter(\n",
    "    summary_df['num_docs'],\n",
    "    summary_df['chunk_size'],\n",
    "    summary_df['num_questions_per_doc'],\n",
    "    c=summary_df['cost_difference'],\n",
    "    cmap='RdBu_r',  # Red-Blue reversed: red = positive (API expensive), blue = negative (Local expensive)\n",
    "    s=100,\n",
    "    alpha=0.7,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "ax1.set_xlabel('Number of Docs', fontsize=11)\n",
    "ax1.set_ylabel('Chunk Size (tokens)', fontsize=11)\n",
    "ax1.set_zlabel('Questions per Doc', fontsize=11)\n",
    "ax1.set_title('3D Parameter Space\\n(Color = Cost Difference: API - Local)', fontsize=13, fontweight='bold')\n",
    "cbar = plt.colorbar(scatter, ax=ax1, label='Cost Difference ($)')\n",
    "cbar.ax.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "# Pairwise plots with cost difference coloring\n",
    "ax2 = fig.add_subplot(222)\n",
    "scatter2 = ax2.scatter(\n",
    "    summary_df['num_docs'],\n",
    "    summary_df['chunk_size'],\n",
    "    c=summary_df['cost_difference'],\n",
    "    cmap='RdBu_r',\n",
    "    s=100,\n",
    "    alpha=0.7,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "ax2.set_xlabel('Number of Docs', fontsize=11)\n",
    "ax2.set_ylabel('Chunk Size (tokens)', fontsize=11)\n",
    "ax2.set_title('Docs vs Chunk Size\\n(Color = Cost Difference)', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(scatter2, ax=ax2, label='Cost Difference ($)')\n",
    "\n",
    "ax3 = fig.add_subplot(223)\n",
    "scatter3 = ax3.scatter(\n",
    "    summary_df['num_docs'],\n",
    "    summary_df['num_questions_per_doc'],\n",
    "    c=summary_df['cost_difference'],\n",
    "    cmap='RdBu_r',\n",
    "    s=100,\n",
    "    alpha=0.7,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "ax3.set_xlabel('Number of Docs', fontsize=11)\n",
    "ax3.set_ylabel('Questions per Doc', fontsize=11)\n",
    "ax3.set_title('Docs vs Questions\\n(Color = Cost Difference)', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(scatter3, ax=ax3, label='Cost Difference ($)')\n",
    "\n",
    "ax4 = fig.add_subplot(224)\n",
    "scatter4 = ax4.scatter(\n",
    "    summary_df['chunk_size'],\n",
    "    summary_df['num_questions_per_doc'],\n",
    "    c=summary_df['cost_difference'],\n",
    "    cmap='RdBu_r',\n",
    "    s=100,\n",
    "    alpha=0.7,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "ax4.set_xlabel('Chunk Size (tokens)', fontsize=11)\n",
    "ax4.set_ylabel('Questions per Doc', fontsize=11)\n",
    "ax4.set_title('Chunk Size vs Questions\\n(Color = Cost Difference)', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(scatter4, ax=ax4, label='Cost Difference ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(experiments_dir / 'nolh_parameter_space_cost_difference.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved parameter space with cost difference to: {experiments_dir / 'nolh_parameter_space_cost_difference.png'}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f) Cost Comparison by Parameter\n",
    "# Three subplots showing cost difference vs each parameter\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Cost difference vs Number of Docs\n",
    "ax1 = axes[0]\n",
    "scatter1 = ax1.scatter(summary_df['num_docs'], summary_df['cost_difference'], \n",
    "                       alpha=0.6, s=80, c=summary_df['cost_difference'], \n",
    "                       cmap='RdBu_r', edgecolors='black', linewidth=0.5)\n",
    "ax1.axhline(y=0, color='black', linestyle='--', linewidth=2, label='Break-even')\n",
    "ax1.set_xlabel('Number of Docs', fontsize=12)\n",
    "ax1.set_ylabel('Cost Difference: API - Local ($)', fontsize=12)\n",
    "ax1.set_title('Cost Difference vs Number of Docs', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "# Add trend line\n",
    "z = np.polyfit(summary_df['num_docs'], summary_df['cost_difference'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax1.plot(summary_df['num_docs'].sort_values(), \n",
    "         p(summary_df['num_docs'].sort_values()), \n",
    "         \"r--\", alpha=0.8, linewidth=2, label=f'Trend: y={z[0]:.6f}x+{z[1]:.4f}')\n",
    "ax1.legend(fontsize=10)\n",
    "\n",
    "# Cost difference vs Chunk Size\n",
    "ax2 = axes[1]\n",
    "scatter2 = ax2.scatter(summary_df['chunk_size'], summary_df['cost_difference'], \n",
    "                       alpha=0.6, s=80, c=summary_df['cost_difference'], \n",
    "                       cmap='RdBu_r', edgecolors='black', linewidth=0.5)\n",
    "ax2.axhline(y=0, color='black', linestyle='--', linewidth=2, label='Break-even')\n",
    "ax2.set_xlabel('Chunk Size (tokens)', fontsize=12)\n",
    "ax2.set_ylabel('Cost Difference: API - Local ($)', fontsize=12)\n",
    "ax2.set_title('Cost Difference vs Chunk Size', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "# Add trend line\n",
    "z = np.polyfit(summary_df['chunk_size'], summary_df['cost_difference'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax2.plot(summary_df['chunk_size'].sort_values(), \n",
    "         p(summary_df['chunk_size'].sort_values()), \n",
    "         \"r--\", alpha=0.8, linewidth=2, label=f'Trend: y={z[0]:.6f}x+{z[1]:.4f}')\n",
    "ax2.legend(fontsize=10)\n",
    "\n",
    "# Cost difference vs Questions per Doc\n",
    "ax3 = axes[2]\n",
    "scatter3 = ax3.scatter(summary_df['num_questions_per_doc'], summary_df['cost_difference'], \n",
    "                      alpha=0.6, s=80, c=summary_df['cost_difference'], \n",
    "                      cmap='RdBu_r', edgecolors='black', linewidth=0.5)\n",
    "ax3.axhline(y=0, color='black', linestyle='--', linewidth=2, label='Break-even')\n",
    "ax3.set_xlabel('Questions per Doc', fontsize=12)\n",
    "ax3.set_ylabel('Cost Difference: API - Local ($)', fontsize=12)\n",
    "ax3.set_title('Cost Difference vs Questions per Doc', fontsize=13, fontweight='bold')\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "# Add trend line\n",
    "z = np.polyfit(summary_df['num_questions_per_doc'], summary_df['cost_difference'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax3.plot(summary_df['num_questions_per_doc'].sort_values(), \n",
    "         p(summary_df['num_questions_per_doc'].sort_values()), \n",
    "         \"r--\", alpha=0.8, linewidth=2, label=f'Trend: y={z[0]:.4f}x+{z[1]:.4f}')\n",
    "ax3.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(experiments_dir / 'nolh_cost_comparison_by_parameter.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved cost comparison by parameter to: {experiments_dir / 'nolh_cost_comparison_by_parameter.png'}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Statistical Analysis\n",
    "\n",
    "Perform correlation analysis and parameter importance ranking to understand which parameters most affect cost, tokens, and runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "# Calculate correlation coefficients between parameters and outcomes\n",
    "# This tells us which parameters are most strongly related to cost, tokens, and runtime\n",
    "\n",
    "# Select parameter columns and outcome columns\n",
    "# Now including both API and local costs, plus cost difference and ratio\n",
    "parameter_cols = ['num_docs', 'chunk_size', 'num_questions_per_doc']\n",
    "outcome_cols = ['cost_api_total', 'cost_local_total', 'cost_difference', 'cost_ratio', \n",
    "                'total_tokens', 'total_runtime_seconds', 'cost_api_per_1k_tokens', 'cost_local_per_1k_tokens']\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_data = []\n",
    "for outcome in outcome_cols:\n",
    "    for param in parameter_cols:\n",
    "        corr = summary_df[param].corr(summary_df[outcome])\n",
    "        correlation_data.append({\n",
    "            'Parameter': param,\n",
    "            'Outcome': outcome,\n",
    "            'Correlation': corr\n",
    "        })\n",
    "\n",
    "correlation_df = pd.DataFrame(correlation_data)\n",
    "correlation_pivot = correlation_df.pivot(index='Parameter', columns='Outcome', values='Correlation')\n",
    "\n",
    "print(\"Correlation Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Values range from -1 (perfect negative correlation) to +1 (perfect positive correlation)\")\n",
    "print(\"Values closer to 0 indicate weak or no correlation\")\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlation_pivot.round(4))\n",
    "\n",
    "# Visualize correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "im = ax.imshow(correlation_pivot.values, cmap='RdYlBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(len(correlation_pivot.columns)))\n",
    "ax.set_yticks(range(len(correlation_pivot.index)))\n",
    "ax.set_xticklabels(correlation_pivot.columns, rotation=45, ha='right')\n",
    "ax.set_yticklabels(correlation_pivot.index)\n",
    "ax.set_title('Parameter-Outcome Correlation Matrix')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(correlation_pivot.index)):\n",
    "    for j in range(len(correlation_pivot.columns)):\n",
    "        text = ax.text(j, i, f'{correlation_pivot.iloc[i, j]:.3f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Correlation Coefficient')\n",
    "plt.tight_layout()\n",
    "plt.savefig(experiments_dir / 'nolh_correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nSaved correlation matrix plot to: {experiments_dir / 'nolh_correlation_matrix.png'}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Importance Ranking\n",
    "# Rank parameters by their absolute correlation with key outcomes\n",
    "# This helps identify which parameters have the biggest impact\n",
    "\n",
    "importance_data = []\n",
    "\n",
    "for outcome in outcome_cols:\n",
    "    # Get correlations for this outcome\n",
    "    outcome_corrs = correlation_df[correlation_df['Outcome'] == outcome].copy()\n",
    "    outcome_corrs['Abs_Correlation'] = outcome_corrs['Correlation'].abs()\n",
    "    outcome_corrs = outcome_corrs.sort_values('Abs_Correlation', ascending=False)\n",
    "    \n",
    "    # Rank parameters\n",
    "    outcome_corrs['Rank'] = range(1, len(outcome_corrs) + 1)\n",
    "    \n",
    "    for _, row in outcome_corrs.iterrows():\n",
    "        importance_data.append({\n",
    "            'Outcome': outcome,\n",
    "            'Parameter': row['Parameter'],\n",
    "            'Correlation': row['Correlation'],\n",
    "            'Abs_Correlation': row['Abs_Correlation'],\n",
    "            'Rank': row['Rank']\n",
    "        })\n",
    "\n",
    "importance_df = pd.DataFrame(importance_data)\n",
    "\n",
    "print(\"\\nParameter Importance Ranking:\")\n",
    "print(\"=\" * 80)\n",
    "for outcome in outcome_cols:\n",
    "    print(f\"\\n{outcome.upper()}:\")\n",
    "    outcome_importance = importance_df[importance_df['Outcome'] == outcome].sort_values('Rank')\n",
    "    for _, row in outcome_importance.iterrows():\n",
    "        direction = \"increases\" if row['Correlation'] > 0 else \"decreases\"\n",
    "        print(f\"  {row['Rank']}. {row['Parameter']}: {row['Correlation']:.4f} ({direction} {outcome})\")\n",
    "\n",
    "# Visualize importance rankings\n",
    "# Create subplots for key outcomes (focusing on cost-related ones)\n",
    "key_outcomes = ['cost_api_total', 'cost_local_total', 'cost_difference', 'cost_ratio']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, outcome in enumerate(key_outcomes):\n",
    "    if idx >= len(axes):\n",
    "        break\n",
    "    ax = axes[idx]\n",
    "    outcome_importance = importance_df[importance_df['Outcome'] == outcome].sort_values('Abs_Correlation', ascending=True)\n",
    "    \n",
    "    colors = ['green' if c > 0 else 'red' for c in outcome_importance['Correlation']]\n",
    "    ax.barh(outcome_importance['Parameter'], outcome_importance['Correlation'], color=colors, alpha=0.7)\n",
    "    ax.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax.set_xlabel('Correlation Coefficient')\n",
    "    ax.set_title(f'Parameter Importance: {outcome}')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(experiments_dir / 'nolh_parameter_importance.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nSaved parameter importance plot to: {experiments_dir / 'nolh_parameter_importance.png'}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Comparison Summary Statistics\n",
    "\n",
    "Calculate and display summary statistics comparing API vs Local pricing models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics: API vs Local Cost Comparison\n",
    "print(\"=\" * 80)\n",
    "print(\"API vs Local Cost Comparison Summary Statistics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate key statistics\n",
    "api_mean = summary_df['cost_api_total'].mean()\n",
    "api_median = summary_df['cost_api_total'].median()\n",
    "api_std = summary_df['cost_api_total'].std()\n",
    "api_min = summary_df['cost_api_total'].min()\n",
    "api_max = summary_df['cost_api_total'].max()\n",
    "\n",
    "local_mean = summary_df['cost_local_total'].mean()\n",
    "local_median = summary_df['cost_local_total'].median()\n",
    "local_std = summary_df['cost_local_total'].std()\n",
    "local_min = summary_df['cost_local_total'].min()\n",
    "local_max = summary_df['cost_local_total'].max()\n",
    "\n",
    "diff_mean = summary_df['cost_difference'].mean()\n",
    "diff_median = summary_df['cost_difference'].median()\n",
    "ratio_mean = summary_df['cost_ratio'].mean()\n",
    "ratio_median = summary_df['cost_ratio'].median()\n",
    "\n",
    "# Count experiments where each pricing model is cheaper\n",
    "api_cheaper = (summary_df['cost_difference'] < 0).sum()\n",
    "local_cheaper = (summary_df['cost_difference'] > 0).sum()\n",
    "equal = (summary_df['cost_difference'] == 0).sum()\n",
    "total = len(summary_df)\n",
    "\n",
    "print(f\"\\n1. Total Cost Statistics:\")\n",
    "print(f\"   API Pricing:\")\n",
    "print(f\"     Mean:   ${api_mean:.4f}\")\n",
    "print(f\"     Median: ${api_median:.4f}\")\n",
    "print(f\"     Std Dev: ${api_std:.4f}\")\n",
    "print(f\"     Range:  ${api_min:.4f} - ${api_max:.4f}\")\n",
    "print(f\"   Local Pricing:\")\n",
    "print(f\"     Mean:   ${local_mean:.4f}\")\n",
    "print(f\"     Median: ${local_median:.4f}\")\n",
    "print(f\"     Std Dev: ${local_std:.4f}\")\n",
    "print(f\"     Range:  ${local_min:.4f} - ${local_max:.4f}\")\n",
    "\n",
    "print(f\"\\n2. Cost Difference (API - Local):\")\n",
    "print(f\"   Mean:   ${diff_mean:.4f}\")\n",
    "print(f\"   Median: ${diff_median:.4f}\")\n",
    "print(f\"   (Positive = API more expensive, Negative = Local more expensive)\")\n",
    "\n",
    "print(f\"\\n3. Cost Ratio (API / Local):\")\n",
    "print(f\"   Mean:   {ratio_mean:.3f}x\")\n",
    "print(f\"   Median: {ratio_median:.3f}x\")\n",
    "print(f\"   (Ratio > 1 = API more expensive, Ratio < 1 = Local more expensive)\")\n",
    "\n",
    "print(f\"\\n4. Pricing Model Comparison:\")\n",
    "print(f\"   API Cheaper:  {api_cheaper} experiments ({api_cheaper/total*100:.1f}%)\")\n",
    "print(f\"   Local Cheaper: {local_cheaper} experiments ({local_cheaper/total*100:.1f}%)\")\n",
    "print(f\"   Equal Cost:   {equal} experiments ({equal/total*100:.1f}%)\")\n",
    "\n",
    "# Calculate savings\n",
    "if api_cheaper > 0:\n",
    "    api_savings = summary_df[summary_df['cost_difference'] < 0]['cost_difference'].abs().sum()\n",
    "    print(f\"\\n5. Potential Savings:\")\n",
    "    print(f\"   If using API when cheaper: ${api_savings:.4f} total savings\")\n",
    "    \n",
    "if local_cheaper > 0:\n",
    "    local_savings = summary_df[summary_df['cost_difference'] > 0]['cost_difference'].sum()\n",
    "    print(f\"   If using Local when cheaper: ${local_savings:.4f} total savings\")\n",
    "\n",
    "# Find parameter ranges where each pricing model is optimal\n",
    "print(f\"\\n6. Optimal Pricing Model by Parameter Ranges:\")\n",
    "\n",
    "# By num_docs\n",
    "api_optimal_docs = summary_df[summary_df['cost_difference'] < 0]['num_docs']\n",
    "local_optimal_docs = summary_df[summary_df['cost_difference'] > 0]['num_docs']\n",
    "if len(api_optimal_docs) > 0:\n",
    "    print(f\"   API optimal when num_docs: {api_optimal_docs.min():.0f} - {api_optimal_docs.max():.0f} (mean: {api_optimal_docs.mean():.1f})\")\n",
    "if len(local_optimal_docs) > 0:\n",
    "    print(f\"   Local optimal when num_docs: {local_optimal_docs.min():.0f} - {local_optimal_docs.max():.0f} (mean: {local_optimal_docs.mean():.1f})\")\n",
    "\n",
    "# By chunk_size\n",
    "api_optimal_chunk = summary_df[summary_df['cost_difference'] < 0]['chunk_size']\n",
    "local_optimal_chunk = summary_df[summary_df['cost_difference'] > 0]['chunk_size']\n",
    "if len(api_optimal_chunk) > 0:\n",
    "    print(f\"   API optimal when chunk_size: {api_optimal_chunk.min():.0f} - {api_optimal_chunk.max():.0f} (mean: {api_optimal_chunk.mean():.1f})\")\n",
    "if len(local_optimal_chunk) > 0:\n",
    "    print(f\"   Local optimal when chunk_size: {local_optimal_chunk.min():.0f} - {local_optimal_chunk.max():.0f} (mean: {local_optimal_chunk.mean():.1f})\")\n",
    "\n",
    "# By num_questions\n",
    "api_optimal_q = summary_df[summary_df['cost_difference'] < 0]['num_questions_per_doc']\n",
    "local_optimal_q = summary_df[summary_df['cost_difference'] > 0]['num_questions_per_doc']\n",
    "if len(api_optimal_q) > 0:\n",
    "    print(f\"   API optimal when num_questions: {api_optimal_q.min():.0f} - {api_optimal_q.max():.0f} (mean: {api_optimal_q.mean():.1f})\")\n",
    "if len(local_optimal_q) > 0:\n",
    "    print(f\"   Local optimal when num_questions: {local_optimal_q.min():.0f} - {local_optimal_q.max():.0f} (mean: {local_optimal_q.mean():.1f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: NOLH Validation\n",
    "\n",
    "Validate the NOLH design by showing sample distribution and verifying space-filling properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample distribution across parameter space\n",
    "# This validates that our NOLH samples are well-distributed\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Histogram of num_docs distribution\n",
    "ax1 = axes[0]\n",
    "ax1.hist(summary_df['num_docs'], bins=15, edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(summary_df['num_docs'].mean(), color='red', linestyle='--', \n",
    "           label=f'Mean: {summary_df[\"num_docs\"].mean():.1f}')\n",
    "ax1.set_xlabel('Number of Docs')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of num_docs Samples')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram of chunk_size distribution\n",
    "ax2 = axes[1]\n",
    "ax2.hist(summary_df['chunk_size'], bins=15, edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(summary_df['chunk_size'].mean(), color='red', linestyle='--', \n",
    "           label=f'Mean: {summary_df[\"chunk_size\"].mean():.1f}')\n",
    "ax2.set_xlabel('Chunk Size (tokens)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Distribution of chunk_size Samples')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram of num_questions_per_doc distribution\n",
    "ax3 = axes[2]\n",
    "ax3.hist(summary_df['num_questions_per_doc'], bins=10, edgecolor='black', alpha=0.7)\n",
    "ax3.axvline(summary_df['num_questions_per_doc'].mean(), color='red', linestyle='--', \n",
    "           label=f'Mean: {summary_df[\"num_questions_per_doc\"].mean():.1f}')\n",
    "ax3.set_xlabel('Questions per Doc')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('Distribution of num_questions_per_doc Samples')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(experiments_dir / 'nolh_sample_distribution.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved sample distribution plot to: {experiments_dir / 'nolh_sample_distribution.png'}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coverage metrics\n",
    "# These metrics validate that our samples cover the parameter space well\n",
    "\n",
    "def calculate_coverage_metrics(df, param_col, param_range):\n",
    "    \"\"\"Calculate coverage metrics for a parameter.\"\"\"\n",
    "    min_val, max_val = param_range\n",
    "    range_size = max_val - min_val\n",
    "    \n",
    "    # Calculate how well the range is covered\n",
    "    actual_min = df[param_col].min()\n",
    "    actual_max = df[param_col].max()\n",
    "    actual_range = actual_max - actual_min\n",
    "    range_coverage = (actual_range / range_size) * 100 if range_size > 0 else 0\n",
    "    \n",
    "    # Calculate spacing (average distance between consecutive sorted values)\n",
    "    sorted_vals = sorted(df[param_col].unique())\n",
    "    if len(sorted_vals) > 1:\n",
    "        spacings = [sorted_vals[i+1] - sorted_vals[i] for i in range(len(sorted_vals)-1)]\n",
    "        avg_spacing = np.mean(spacings)\n",
    "        ideal_spacing = range_size / (len(sorted_vals) - 1) if len(sorted_vals) > 1 else 0\n",
    "        spacing_efficiency = (ideal_spacing / avg_spacing * 100) if avg_spacing > 0 else 0\n",
    "    else:\n",
    "        avg_spacing = 0\n",
    "        spacing_efficiency = 0\n",
    "    \n",
    "    return {\n",
    "        'parameter': param_col,\n",
    "        'min_value': actual_min,\n",
    "        'max_value': actual_max,\n",
    "        'range_coverage_pct': range_coverage,\n",
    "        'unique_values': len(sorted_vals),\n",
    "        'avg_spacing': avg_spacing,\n",
    "        'spacing_efficiency_pct': spacing_efficiency\n",
    "    }\n",
    "\n",
    "# Calculate coverage for each parameter\n",
    "coverage_metrics = []\n",
    "for param, param_range in parameter_ranges.items():\n",
    "    metrics = calculate_coverage_metrics(summary_df, param, param_range)\n",
    "    coverage_metrics.append(metrics)\n",
    "\n",
    "coverage_df = pd.DataFrame(coverage_metrics)\n",
    "\n",
    "print(\"NOLH Design Coverage Metrics:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nThese metrics validate that our samples are well-distributed:\")\n",
    "print(\"  - Range Coverage: % of parameter range actually covered by samples\")\n",
    "print(\"  - Unique Values: Number of distinct values sampled\")\n",
    "print(\"  - Spacing Efficiency: How evenly spaced the samples are (100% = perfectly even)\")\n",
    "print(\"\\nCoverage Summary:\")\n",
    "print(coverage_df.to_string(index=False))\n",
    "\n",
    "# Calculate overall design quality score\n",
    "# This is a simple metric combining coverage and spacing\n",
    "overall_quality = (\n",
    "    coverage_df['range_coverage_pct'].mean() * 0.5 +\n",
    "    coverage_df['spacing_efficiency_pct'].mean() * 0.5\n",
    ")\n",
    "\n",
    "print(f\"\\nOverall Design Quality Score: {overall_quality:.2f}%\")\n",
    "print(\"(Higher is better - indicates good space-filling properties)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: CSV Exports\n",
    "\n",
    "Export all important data to CSV files for easy analysis in Excel, Google Sheets, or other tools.\n",
    "\n",
    "This section creates multiple CSV files with different views of the experimental data:\n",
    "1. **Full Experiment Summary** - Complete results from all experiments\n",
    "2. **Coverage Metrics** - NOLH design quality metrics\n",
    "3. **Model Performance Comparison** - Aggregated metrics by model\n",
    "4. **Cost Analysis Summary** - Cost breakdowns and comparisons\n",
    "5. **Parameter Sensitivity** - How parameters affect costs and performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CSV Export Section\n",
    "# ============================================================================\n",
    "# Export all important data to CSV files for spreadsheet analysis\n",
    "# Each CSV file focuses on a different aspect of the experimental results\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Create exports directory if it doesn't exist\n",
    "exports_dir = experiments_dir / \"csv_exports\"\n",
    "exports_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Get current timestamp for export metadata\n",
    "export_timestamp = datetime.now().isoformat()\n",
    "export_timestamp_readable = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CSV Export Section\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Export timestamp: {export_timestamp_readable}\")\n",
    "print(f\"Export directory: {exports_dir}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Full Experiment Summary\n",
    "\n",
    "Complete results from all experiments with all metrics and costs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Full Experiment Summary\n",
    "# This is the complete dataset with all experiment results\n",
    "# Includes all metrics, costs, and configuration parameters\n",
    "\n",
    "# Ensure summary_df exists and has data\n",
    "if 'summary_df' in locals() and not summary_df.empty:\n",
    "    # Add export timestamp column\n",
    "    summary_df_export = summary_df.copy()\n",
    "    summary_df_export['export_timestamp'] = export_timestamp\n",
    "    summary_df_export['export_timestamp_readable'] = export_timestamp_readable\n",
    "    \n",
    "    # Reorder columns to put timestamps first (if they exist)\n",
    "    if 'experiment_timestamp' in summary_df_export.columns:\n",
    "        cols = ['experiment_timestamp', 'experiment_timestamp_readable', 'export_timestamp', 'export_timestamp_readable'] + \\\n",
    "               [c for c in summary_df_export.columns if c not in ['experiment_timestamp', 'experiment_timestamp_readable', 'export_timestamp', 'export_timestamp_readable']]\n",
    "        summary_df_export = summary_df_export[cols]\n",
    "    else:\n",
    "        cols = ['export_timestamp', 'export_timestamp_readable'] + \\\n",
    "               [c for c in summary_df_export.columns if c not in ['export_timestamp', 'export_timestamp_readable']]\n",
    "        summary_df_export = summary_df_export[cols]\n",
    "    \n",
    "    # Save to CSV\n",
    "    full_summary_path = exports_dir / \"01_full_experiment_summary.csv\"\n",
    "    summary_df_export.to_csv(full_summary_path, index=False)\n",
    "    print(f\"âœ… 1. Full Experiment Summary\")\n",
    "    print(f\"   File: {full_summary_path.name}\")\n",
    "    print(f\"   Rows: {len(summary_df_export)}, Columns: {len(summary_df_export.columns)}\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"âš ï¸  summary_df not available. Run experiment execution section first.\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Coverage Metrics\n",
    "\n",
    "NOLH design quality metrics showing how well the parameter space was covered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Coverage Metrics\n",
    "# NOLH design quality metrics\n",
    "\n",
    "if 'coverage_df' in locals() and not coverage_df.empty:\n",
    "    coverage_df_export = coverage_df.copy()\n",
    "    coverage_df_export['export_timestamp'] = export_timestamp\n",
    "    coverage_df_export['export_timestamp_readable'] = export_timestamp_readable\n",
    "    \n",
    "    # Reorder columns\n",
    "    cols = ['export_timestamp', 'export_timestamp_readable'] + \\\n",
    "           [c for c in coverage_df_export.columns if c not in ['export_timestamp', 'export_timestamp_readable']]\n",
    "    coverage_df_export = coverage_df_export[cols]\n",
    "    \n",
    "    coverage_path = exports_dir / \"02_nolh_coverage_metrics.csv\"\n",
    "    coverage_df_export.to_csv(coverage_path, index=False)\n",
    "    print(f\"âœ… 2. Coverage Metrics\")\n",
    "    print(f\"   File: {coverage_path.name}\")\n",
    "    print(f\"   Rows: {len(coverage_df_export)}, Columns: {len(coverage_df_export.columns)}\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"âš ï¸  coverage_df not available. Run coverage analysis section first.\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Performance Comparison\n",
    "\n",
    "Aggregated performance metrics grouped by generation and embedding models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model Performance Comparison\n",
    "# Aggregate metrics by generation model and embedding model\n",
    "\n",
    "if 'summary_df' in locals() and not summary_df.empty:\n",
    "    # Group by generation model\n",
    "    gen_model_stats = summary_df.groupby('gen_model').agg({\n",
    "        'total_tokens': ['count', 'mean', 'sum'],\n",
    "        'total_runtime_seconds': ['mean', 'sum'],\n",
    "        'tokens_per_second': 'mean',\n",
    "        'cost_api_total': ['mean', 'sum', 'min', 'max'],\n",
    "        'cost_local_total': ['mean', 'sum', 'min', 'max'],\n",
    "        'cost_difference': 'mean',\n",
    "        'cost_ratio': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    # Flatten column names\n",
    "    gen_model_stats.columns = ['_'.join(col).strip() for col in gen_model_stats.columns.values]\n",
    "    gen_model_stats = gen_model_stats.reset_index()\n",
    "    gen_model_stats['export_timestamp'] = export_timestamp\n",
    "    gen_model_stats['export_timestamp_readable'] = export_timestamp_readable\n",
    "    \n",
    "    # Reorder columns\n",
    "    cols = ['export_timestamp', 'export_timestamp_readable', 'gen_model'] + \\\n",
    "           [c for c in gen_model_stats.columns if c not in ['export_timestamp', 'export_timestamp_readable', 'gen_model']]\n",
    "    gen_model_stats = gen_model_stats[cols]\n",
    "    \n",
    "    gen_model_path = exports_dir / \"03_model_performance_by_gen_model.csv\"\n",
    "    gen_model_stats.to_csv(gen_model_path, index=False)\n",
    "    print(f\"âœ… 3a. Generation Model Performance\")\n",
    "    print(f\"   File: {gen_model_path.name}\")\n",
    "    print(f\"   Rows: {len(gen_model_stats)}, Columns: {len(gen_model_stats.columns)}\")\n",
    "    \n",
    "    # Group by embedding model\n",
    "    embed_model_stats = summary_df.groupby('embed_model').agg({\n",
    "        'total_tokens': ['count', 'mean', 'sum'],\n",
    "        'total_runtime_seconds': ['mean', 'sum'],\n",
    "        'tokens_per_second': 'mean',\n",
    "        'cost_api_total': ['mean', 'sum', 'min', 'max'],\n",
    "        'cost_local_total': ['mean', 'sum', 'min', 'max'],\n",
    "        'cost_difference': 'mean',\n",
    "        'cost_ratio': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    # Flatten column names\n",
    "    embed_model_stats.columns = ['_'.join(col).strip() for col in embed_model_stats.columns.values]\n",
    "    embed_model_stats = embed_model_stats.reset_index()\n",
    "    embed_model_stats['export_timestamp'] = export_timestamp\n",
    "    embed_model_stats['export_timestamp_readable'] = export_timestamp_readable\n",
    "    \n",
    "    # Reorder columns\n",
    "    cols = ['export_timestamp', 'export_timestamp_readable', 'embed_model'] + \\\n",
    "           [c for c in embed_model_stats.columns if c not in ['export_timestamp', 'export_timestamp_readable', 'embed_model']]\n",
    "    embed_model_stats = embed_model_stats[cols]\n",
    "    \n",
    "    embed_model_path = exports_dir / \"03_model_performance_by_embed_model.csv\"\n",
    "    embed_model_stats.to_csv(embed_model_path, index=False)\n",
    "    print(f\"âœ… 3b. Embedding Model Performance\")\n",
    "    print(f\"   File: {embed_model_path.name}\")\n",
    "    print(f\"   Rows: {len(embed_model_stats)}, Columns: {len(embed_model_stats.columns)}\")\n",
    "    \n",
    "    # Group by model combination\n",
    "    model_combo_stats = summary_df.groupby(['gen_model', 'embed_model']).agg({\n",
    "        'total_tokens': ['count', 'mean', 'sum'],\n",
    "        'total_runtime_seconds': ['mean', 'sum'],\n",
    "        'tokens_per_second': 'mean',\n",
    "        'cost_api_total': ['mean', 'sum', 'min', 'max'],\n",
    "        'cost_local_total': ['mean', 'sum', 'min', 'max'],\n",
    "        'cost_difference': 'mean',\n",
    "        'cost_ratio': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    # Flatten column names\n",
    "    model_combo_stats.columns = ['_'.join(col).strip() for col in model_combo_stats.columns.values]\n",
    "    model_combo_stats = model_combo_stats.reset_index()\n",
    "    model_combo_stats['export_timestamp'] = export_timestamp\n",
    "    model_combo_stats['export_timestamp_readable'] = export_timestamp_readable\n",
    "    \n",
    "    # Reorder columns\n",
    "    cols = ['export_timestamp', 'export_timestamp_readable', 'gen_model', 'embed_model'] + \\\n",
    "           [c for c in model_combo_stats.columns if c not in ['export_timestamp', 'export_timestamp_readable', 'gen_model', 'embed_model']]\n",
    "    model_combo_stats = model_combo_stats[cols]\n",
    "    \n",
    "    model_combo_path = exports_dir / \"03_model_performance_by_combination.csv\"\n",
    "    model_combo_stats.to_csv(model_combo_path, index=False)\n",
    "    print(f\"âœ… 3c. Model Combination Performance\")\n",
    "    print(f\"   File: {model_combo_path.name}\")\n",
    "    print(f\"   Rows: {len(model_combo_stats)}, Columns: {len(model_combo_stats.columns)}\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"âš ï¸  summary_df not available. Run experiment execution section first.\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cost Analysis Summary\n",
    "\n",
    "Detailed cost breakdowns comparing API vs local pricing models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Cost Analysis Summary\n",
    "# Detailed cost comparisons and breakdowns\n",
    "\n",
    "if 'summary_df' in locals() and not summary_df.empty and 'cost_api_total' in summary_df.columns:\n",
    "    # Create cost comparison summary\n",
    "    cost_summary = {\n",
    "        'export_timestamp': [export_timestamp],\n",
    "        'export_timestamp_readable': [export_timestamp_readable],\n",
    "        'total_experiments': [len(summary_df)],\n",
    "        # API costs\n",
    "        'api_total_cost_sum': [summary_df['cost_api_total'].sum()],\n",
    "        'api_total_cost_mean': [summary_df['cost_api_total'].mean()],\n",
    "        'api_total_cost_min': [summary_df['cost_api_total'].min()],\n",
    "        'api_total_cost_max': [summary_df['cost_api_total'].max()],\n",
    "        'api_cost_per_1k_tokens_mean': [summary_df['cost_api_per_1k_tokens'].mean()],\n",
    "        # Local costs\n",
    "        'local_total_cost_sum': [summary_df['cost_local_total'].sum()],\n",
    "        'local_total_cost_mean': [summary_df['cost_local_total'].mean()],\n",
    "        'local_total_cost_min': [summary_df['cost_local_total'].min()],\n",
    "        'local_total_cost_max': [summary_df['cost_local_total'].max()],\n",
    "        'local_cost_per_1k_tokens_mean': [summary_df['cost_local_per_1k_tokens'].mean()],\n",
    "        # Cost comparison\n",
    "        'cost_difference_mean': [summary_df['cost_difference'].mean()],\n",
    "        'cost_difference_sum': [summary_df['cost_difference'].sum()],\n",
    "        'cost_ratio_mean': [summary_df['cost_ratio'].mean()],\n",
    "        'api_cheaper_count': [(summary_df['cost_difference'] < 0).sum()],\n",
    "        'local_cheaper_count': [(summary_df['cost_difference'] > 0).sum()],\n",
    "        'api_cheaper_pct': [(summary_df['cost_difference'] < 0).sum() / len(summary_df) * 100],\n",
    "        'local_cheaper_pct': [(summary_df['cost_difference'] > 0).sum() / len(summary_df) * 100]\n",
    "    }\n",
    "    \n",
    "    cost_summary_df = pd.DataFrame(cost_summary)\n",
    "    cost_summary_path = exports_dir / \"04_cost_analysis_summary.csv\"\n",
    "    cost_summary_df.to_csv(cost_summary_path, index=False)\n",
    "    print(f\"âœ… 4a. Cost Analysis Summary\")\n",
    "    print(f\"   File: {cost_summary_path.name}\")\n",
    "    print(f\"   Rows: {len(cost_summary_df)}, Columns: {len(cost_summary_df.columns)}\")\n",
    "    \n",
    "    # Cost breakdown by parameter ranges\n",
    "    # Group by num_docs ranges\n",
    "    summary_df['num_docs_range'] = pd.cut(summary_df['num_docs'], \n",
    "                                         bins=[0, 10, 20, 30, 40, 50, 100],\n",
    "                                         labels=['1-10', '11-20', '21-30', '31-40', '41-50', '50+'])\n",
    "    \n",
    "    cost_by_docs = summary_df.groupby('num_docs_range').agg({\n",
    "        'cost_api_total': ['mean', 'sum', 'count'],\n",
    "        'cost_local_total': ['mean', 'sum'],\n",
    "        'cost_difference': 'mean',\n",
    "        'cost_ratio': 'mean'\n",
    "    }).round(4)\n",
    "    cost_by_docs.columns = ['_'.join(col).strip() for col in cost_by_docs.columns.values]\n",
    "    cost_by_docs = cost_by_docs.reset_index()\n",
    "    cost_by_docs['export_timestamp'] = export_timestamp\n",
    "    cost_by_docs['export_timestamp_readable'] = export_timestamp_readable\n",
    "    \n",
    "    cols = ['export_timestamp', 'export_timestamp_readable', 'num_docs_range'] + \\\n",
    "           [c for c in cost_by_docs.columns if c not in ['export_timestamp', 'export_timestamp_readable', 'num_docs_range']]\n",
    "    cost_by_docs = cost_by_docs[cols]\n",
    "    \n",
    "    cost_by_docs_path = exports_dir / \"04_cost_by_num_docs.csv\"\n",
    "    cost_by_docs.to_csv(cost_by_docs_path, index=False)\n",
    "    print(f\"âœ… 4b. Cost by Number of Documents\")\n",
    "    print(f\"   File: {cost_by_docs_path.name}\")\n",
    "    \n",
    "    # Group by chunk_size ranges\n",
    "    summary_df['chunk_size_range'] = pd.cut(summary_df['chunk_size'],\n",
    "                                           bins=[0, 500, 1000, 2000, 3000, 4000, 5000],\n",
    "                                           labels=['0-500', '501-1000', '1001-2000', '2001-3000', '3001-4000', '4000+'])\n",
    "    \n",
    "    cost_by_chunk = summary_df.groupby('chunk_size_range').agg({\n",
    "        'cost_api_total': ['mean', 'sum', 'count'],\n",
    "        'cost_local_total': ['mean', 'sum'],\n",
    "        'cost_difference': 'mean',\n",
    "        'cost_ratio': 'mean'\n",
    "    }).round(4)\n",
    "    cost_by_chunk.columns = ['_'.join(col).strip() for col in cost_by_chunk.columns.values]\n",
    "    cost_by_chunk = cost_by_chunk.reset_index()\n",
    "    cost_by_chunk['export_timestamp'] = export_timestamp\n",
    "    cost_by_chunk['export_timestamp_readable'] = export_timestamp_readable\n",
    "    \n",
    "    cols = ['export_timestamp', 'export_timestamp_readable', 'chunk_size_range'] + \\\n",
    "           [c for c in cost_by_chunk.columns if c not in ['export_timestamp', 'export_timestamp_readable', 'chunk_size_range']]\n",
    "    cost_by_chunk = cost_by_chunk[cols]\n",
    "    \n",
    "    cost_by_chunk_path = exports_dir / \"04_cost_by_chunk_size.csv\"\n",
    "    cost_by_chunk.to_csv(cost_by_chunk_path, index=False)\n",
    "    print(f\"âœ… 4c. Cost by Chunk Size\")\n",
    "    print(f\"   File: {cost_by_chunk_path.name}\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"âš ï¸  summary_df not available or missing cost columns. Run experiment execution section first.\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Parameter Sensitivity Analysis\n",
    "\n",
    "How different parameters affect costs, performance, and token usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Parameter Sensitivity Analysis\n",
    "# Analyze how parameters affect costs and performance\n",
    "\n",
    "if 'summary_df' in locals() and not summary_df.empty:\n",
    "    # Calculate correlations between parameters and outcomes\n",
    "    numeric_cols = ['num_docs', 'chunk_size', 'num_questions_per_doc', \n",
    "                    'total_tokens', 'total_runtime_seconds', 'tokens_per_second',\n",
    "                    'cost_api_total', 'cost_local_total', 'cost_difference', 'cost_ratio']\n",
    "    \n",
    "    # Filter to only columns that exist\n",
    "    available_cols = [c for c in numeric_cols if c in summary_df.columns]\n",
    "    correlation_df = summary_df[available_cols].corr()\n",
    "    \n",
    "    # Add export timestamp\n",
    "    correlation_df['export_timestamp'] = export_timestamp\n",
    "    correlation_df['export_timestamp_readable'] = export_timestamp_readable\n",
    "    correlation_df = correlation_df.reset_index()\n",
    "    correlation_df.rename(columns={'index': 'parameter'}, inplace=True)\n",
    "    \n",
    "    # Reorder columns\n",
    "    cols = ['export_timestamp', 'export_timestamp_readable', 'parameter'] + \\\n",
    "           [c for c in correlation_df.columns if c not in ['export_timestamp', 'export_timestamp_readable', 'parameter']]\n",
    "    correlation_df = correlation_df[cols]\n",
    "    \n",
    "    correlation_path = exports_dir / \"05_parameter_correlations.csv\"\n",
    "    correlation_df.to_csv(correlation_path, index=False)\n",
    "    print(f\"âœ… 5a. Parameter Correlations\")\n",
    "    print(f\"   File: {correlation_path.name}\")\n",
    "    print(f\"   Shows how parameters correlate with costs and performance\")\n",
    "    \n",
    "    # Parameter impact summary\n",
    "    param_impact = []\n",
    "    \n",
    "    for param in ['num_docs', 'chunk_size', 'num_questions_per_doc']:\n",
    "        if param in summary_df.columns:\n",
    "            # Calculate how this parameter affects costs\n",
    "            param_corr_cost_api = summary_df[param].corr(summary_df['cost_api_total']) if 'cost_api_total' in summary_df.columns else 0\n",
    "            param_corr_cost_local = summary_df[param].corr(summary_df['cost_local_total']) if 'cost_local_total' in summary_df.columns else 0\n",
    "            param_corr_tokens = summary_df[param].corr(summary_df['total_tokens']) if 'total_tokens' in summary_df.columns else 0\n",
    "            param_corr_runtime = summary_df[param].corr(summary_df['total_runtime_seconds']) if 'total_runtime_seconds' in summary_df.columns else 0\n",
    "            \n",
    "            param_impact.append({\n",
    "                'export_timestamp': export_timestamp,\n",
    "                'export_timestamp_readable': export_timestamp_readable,\n",
    "                'parameter': param,\n",
    "                'correlation_with_cost_api': round(param_corr_cost_api, 4),\n",
    "                'correlation_with_cost_local': round(param_corr_cost_local, 4),\n",
    "                'correlation_with_total_tokens': round(param_corr_tokens, 4),\n",
    "                'correlation_with_runtime': round(param_corr_runtime, 4),\n",
    "                'parameter_min': summary_df[param].min(),\n",
    "                'parameter_max': summary_df[param].max(),\n",
    "                'parameter_mean': round(summary_df[param].mean(), 2)\n",
    "            })\n",
    "    \n",
    "    param_impact_df = pd.DataFrame(param_impact)\n",
    "    param_impact_path = exports_dir / \"05_parameter_impact.csv\"\n",
    "    param_impact_df.to_csv(param_impact_path, index=False)\n",
    "    print(f\"âœ… 5b. Parameter Impact Analysis\")\n",
    "    print(f\"   File: {param_impact_path.name}\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"âš ï¸  summary_df not available. Run experiment execution section first.\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Export Summary\n",
    "\n",
    "Summary of all exported files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Export Summary\n",
    "# List all exported CSV files with their details\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CSV Export Summary\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# List all CSV files in exports directory\n",
    "csv_files = sorted(exports_dir.glob(\"*.csv\"))\n",
    "\n",
    "if csv_files:\n",
    "    print(f\"âœ… Successfully exported {len(csv_files)} CSV files:\")\n",
    "    print()\n",
    "    \n",
    "    for i, csv_file in enumerate(csv_files, 1):\n",
    "        # Get file size\n",
    "        file_size = csv_file.stat().st_size\n",
    "        file_size_kb = file_size / 1024\n",
    "        \n",
    "        # Try to read and count rows\n",
    "        try:\n",
    "            df_temp = pd.read_csv(csv_file)\n",
    "            num_rows = len(df_temp)\n",
    "            num_cols = len(df_temp.columns)\n",
    "            print(f\"  {i}. {csv_file.name}\")\n",
    "            print(f\"     Size: {file_size_kb:.2f} KB\")\n",
    "            print(f\"     Rows: {num_rows:,}, Columns: {num_cols}\")\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"  {i}. {csv_file.name}\")\n",
    "            print(f\"     Size: {file_size_kb:.2f} KB\")\n",
    "            print(f\"     (Could not read file: {e})\")\n",
    "            print()\n",
    "    \n",
    "    print(f\"All files saved to: {exports_dir}\")\n",
    "    print()\n",
    "    print(\"ðŸ’¡ Tip: These CSV files can be opened in Excel, Google Sheets, or any spreadsheet application\")\n",
    "    print(\"   for further analysis, filtering, and visualization.\")\n",
    "else:\n",
    "    print(\"âš ï¸  No CSV files were exported. Check that all required data is available.\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
