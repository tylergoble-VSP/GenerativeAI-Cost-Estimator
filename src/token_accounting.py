"""
Token accounting module for accurate token counting.

This module provides token counting functionality for use with Ollama.
Since we're using Ollama (not Hugging Face directly), we use a simple
estimation method that approximates token counts without requiring
authentication.

Token counting is important for:
- Estimating costs (many LLM APIs charge per token)
- Ensuring prompts fit within model limits
- Tracking usage accurately

Note: This uses a simple word-based estimation. For more accurate counts,
you can check the 'usage' field in Ollama's API responses.
"""

import re
from typing import Dict, Optional


def _estimate_tokens_simple(text: str) -> int:
    """
    Estimate tokens using a simple word-based method.
    
    This is a fallback method that doesn't require any external dependencies
    or authentication. It approximates tokens by counting words and punctuation.
    
    The estimation: ~1 token per word + ~0.5 tokens per punctuation mark
    This is a rough approximation - actual token counts may vary.
    
    Args:
        text: The text to estimate tokens for
    
    Returns:
        Integer: Estimated number of tokens
    """
    # Count words (split on whitespace)
    words = len(text.split())
    
    # Count punctuation marks (common punctuation)
    punctuation = len(re.findall(r'[.,!?;:()\[\]{}\-"\']', text))
    
    # Rough estimation: 1 token per word + 0.5 tokens per punctuation
    # Round up to be conservative
    estimated_tokens = words + int(punctuation * 0.5)
    
    return estimated_tokens


def _count_tokens_with_tiktoken(text: str) -> Optional[int]:
    """
    Try to count tokens using tiktoken (if available).
    
    tiktoken is a fast tokenizer that doesn't require authentication.
    We use the cl100k_base encoding which is similar to GPT models.
    While not exactly Gemma's tokenization, it's a good approximation.
    
    Args:
        text: The text to count tokens for
    
    Returns:
        Integer token count if tiktoken is available, None otherwise
    """
    try:
        import tiktoken
        # Use cl100k_base encoding (used by GPT-4, similar tokenization style)
        # This is a reasonable approximation for Gemma
        encoding = tiktoken.get_encoding("cl100k_base")
        return len(encoding.encode(text))
    except ImportError:
        # tiktoken not installed, return None to use fallback
        return None


def count_tokens(text: str, model_name: Optional[str] = None, token: Optional[str] = None) -> int:
    """
    Count the number of tokens in a text string.
    
    Tokens are the basic units that language models work with.
    A token can be a word, part of a word, or punctuation.
    For example, "Hello world!" might be 3 tokens: ["Hello", " world", "!"]
    
    Since we're using Ollama (not Hugging Face directly), this function
    uses tiktoken if available (no authentication required), or falls back
    to a simple word-based estimation.
    
    For the most accurate token counts, check the 'usage' field in Ollama's
    API responses, which provides actual token counts from the model.
    
    Args:
        text: The text string to count tokens in
        model_name: Not used (kept for compatibility)
        token: Not used (kept for compatibility)
    
    Returns:
        Integer: The estimated number of tokens in the text
    
    Example:
        >>> count_tokens("Hello, world!")
        3  # (example - actual count depends on method used)
    """
    # Try tiktoken first (fast and accurate, no auth needed)
    tiktoken_count = _count_tokens_with_tiktoken(text)
    if tiktoken_count is not None:
        return tiktoken_count
    
    # Fallback to simple estimation
    return _estimate_tokens_simple(text)


def count_prompt_and_response(prompt: str, response: str, 
                              model_name: Optional[str] = None,
                              token: Optional[str] = None) -> Dict[str, int]:
    """
    Count tokens in both prompt and response separately.
    
    This is useful for tracking:
    - How many tokens you sent to the model (prompt tokens)
    - How many tokens the model generated (response tokens)
    - Total tokens used in the interaction
    
    Args:
        prompt: The input text sent to the model (user's question/instruction)
        response: The text generated by the model (model's answer)
        model_name: Not used (kept for compatibility)
        token: Not used (kept for compatibility)
    
    Returns:
        Dictionary with keys:
            - 'prompt_tokens': Number of tokens in the prompt
            - 'response_tokens': Number of tokens in the response
            - 'total_tokens': Sum of prompt and response tokens
    
    Example:
        >>> count_prompt_and_response("What is AI?", "AI is artificial intelligence.")
        {'prompt_tokens': 4, 'response_tokens': 6, 'total_tokens': 10}
    """
    # Count tokens in the prompt (input)
    prompt_tokens = count_tokens(prompt, model_name, token=token)
    
    # Count tokens in the response (output)
    response_tokens = count_tokens(response, model_name, token=token)
    
    # Calculate total tokens
    total_tokens = prompt_tokens + response_tokens
    
    # Return as a dictionary for easy access
    return {
        "prompt_tokens": prompt_tokens,
        "response_tokens": response_tokens,
        "total_tokens": total_tokens
    }


def estimate_embedding_tokens(text: str, model_name: Optional[str] = None,
                              token: Optional[str] = None) -> int:
    """
    Estimate tokens for embedding calls.
    
    For embedding models, we typically only count input tokens
    (the text being embedded), not output tokens (the embedding vector).
    
    This is a convenience function that's the same as count_tokens,
    but makes the intent clearer when used for embedding operations.
    
    Args:
        text: The text to embed
        model_name: Not used (kept for compatibility)
        token: Not used (kept for compatibility)
    
    Returns:
        Integer: Number of tokens in the input text
    """
    return count_tokens(text, model_name, token=token)

